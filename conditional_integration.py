# -*- coding: utf-8 -*-
"""Conditional Integration

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WMFKL46nzn_g0MmkrEEyJsudBFKVpSEM
"""

##########################
##########################
##########################
##########################
#C XVAE Type 1
##########################
##########################
##########################
##########################

def CXVAET1(dataset1, dataset2, target, n_runs, labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)

        # Reset the index if needed
        y1 = y1.reset_index(drop=True)
        y2 = y2.reset_index(drop=True)

        from sklearn.model_selection import train_test_split

        # Define your datasets
        X_datasets = [X1, X2]
        y_datasets = [y1, y2]

        # Dictionaries to store the train and test sets with indices starting from 1
        XTrain = {}
        XTest = {}
        YTrain = {}
        YTest = {}

        # Loop through each dataset and split into train and test sets
        for i in range(len(X_datasets)):
            y_labels = np.argmax(y_datasets[i], axis=1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_datasets[i],
                y_datasets[i],
                test_size=0.3,
                stratify=y_labels,
                random_state=seed
            )

            # Store the results in dictionaries, using i+1 as the key
            # Store the results in dictionaries using i+1 as the key to match numbering
            XTrain[f'XTrain_{i+1}'] = X_train
            XTest[f'XTest_{i+1}'] = X_test
            YTrain[f'YTrain_{i+1}'] = y_train
            YTest[f'YTest_{i+1}'] = y_test

        for i in range(1, len(XTrain) + 1):
            globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
            globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
            globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
            globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary



        # Loop to process datasets 1 and 2
        for i in range(1, 3):


            # Convert to matrix
            globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
            globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

            # One hot encode Y and convert to matrix
            globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
            globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

            # Get number of features
            globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation, name='encoder_input_2')(x_ini_2)
            xe_2 = Dropout(k, name='encoder_dropout_2')(xe_2)


            ############################

            ##############################

            # Integration for encoder
            xe = Concatenate(axis=-1)([xe_1, xe_2])

            ##############################

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            xe = Concatenate(axis=1)([xe, labels])

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)



            xd2 = Concatenate(axis=1)([xd1, labels])

            xd3 = Dense(num_neurons, activation=f_activation)(xd2)
            xd4 = Dropout(k)(xd3)


            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation, name='decoder_output_2')(xd4)
            xd_ds2 = Dropout(k, name='decoder_dropout_2')(xd_ds2)




            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)



            return Model(inputs=[x_ini_1, x_ini_2, labels], outputs=[xhat_1,xhat_2, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, xhat_2, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)

            return KLD + BCE_1 + BCE_2, KLD, BCE_1, BCE_2

        for i in range(1, 3):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons



        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop

        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1
        # Custom training loop

        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, xhat_2, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2 = cvae_loss([batch_xs1, batch_xs1_2], [xhat_1, xhat_2, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))



        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2

        _,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2, YTrainCLMHE_1])
        _,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest['YTest_1']
        y_train = YTrain['YTrain_1']
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model
        if len(y_test.shape) > 1 and y_test.shape[1] > 1:
            y_test = np.argmax(y_test, axis=1)

        if len(y_pred_RF.shape) > 1 and y_pred_RF.shape[1] > 1:
            y_pred_RF = np.argmax(y_pred_RF, axis=1)
        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test)

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        from sklearn.linear_model import LogisticRegression


        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test, y_pred_LR)

        accuracy_LR = accuracy_score(y_test, y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test, y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from C XVAE Type 1 Architecture"
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results

##########################
##########################
##########################
##########################
#C XVAE Type 2
##########################
##########################
##########################
##########################

def CXVAET2(dataset1, dataset2, target, n_runs, labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)

        # Reset the index if needed
        y1 = y1.reset_index(drop=True)
        y2 = y2.reset_index(drop=True)

        from sklearn.model_selection import train_test_split

        # Define your datasets
        X_datasets = [X1, X2]
        y_datasets = [y1, y2]

        # Dictionaries to store the train and test sets with indices starting from 1
        XTrain = {}
        XTest = {}
        YTrain = {}
        YTest = {}

        # Loop through each dataset and split into train and test sets
        for i in range(len(X_datasets)):
            y_labels = np.argmax(y_datasets[i], axis=1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_datasets[i],
                y_datasets[i],
                test_size=0.3,
                stratify=y_labels,
                random_state=seed
            )

            # Store the results in dictionaries, using i+1 as the key
            # Store the results in dictionaries using i+1 as the key to match numbering
            XTrain[f'XTrain_{i+1}'] = X_train
            XTest[f'XTest_{i+1}'] = X_test
            YTrain[f'YTrain_{i+1}'] = y_train
            YTest[f'YTest_{i+1}'] = y_test

        for i in range(1, len(XTrain) + 1):
            globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
            globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
            globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
            globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary



        # Loop to process datasets 1 and 2
        for i in range(1, 3):


            # Convert to matrix
            globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
            globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

            # One hot encode Y and convert to matrix
            globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
            globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

            # Get number of features
            globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation, name='encoder_input_2')(x_ini_2)
            xe_2 = Dropout(k, name='encoder_dropout_2')(xe_2)


            ############################

            ##############################

            # Integration for encoder
            xe = Concatenate(axis=-1)([xe_1, xe_2, labels])

            ##############################

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)


            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################

            z = Concatenate(axis=1)([z, labels])
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)

            xd3 = Dense(num_neurons, activation=f_activation)(xd1)
            xd4 = Dropout(k)(xd3)


            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation, name='decoder_output_2')(xd4)
            xd_ds2 = Dropout(k, name='decoder_dropout_2')(xd_ds2)




            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)



            return Model(inputs=[x_ini_1, x_ini_2, labels], outputs=[xhat_1,xhat_2, z_mu, z_log_sigma])


        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, xhat_2, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)

            return KLD + BCE_1 + BCE_2, KLD, BCE_1, BCE_2

        for i in range(1, 3):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons



        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop

        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1
        # Custom training loop

        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, xhat_2, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2 = cvae_loss([batch_xs1, batch_xs1_2], [xhat_1, xhat_2, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))



        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2

        _,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2, YTrainCLMHE_1])
        _,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest['YTest_1']
        y_train = YTrain['YTrain_1']
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model
        if len(y_test.shape) > 1 and y_test.shape[1] > 1:
            y_test = np.argmax(y_test, axis=1)

        if len(y_pred_RF.shape) > 1 and y_pred_RF.shape[1] > 1:
            y_pred_RF = np.argmax(y_pred_RF, axis=1)
        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test)

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        from sklearn.linear_model import LogisticRegression


        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test, y_pred_LR)

        accuracy_LR = accuracy_score(y_test, y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test, y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from C XVAE Type 2 Architecture"
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results



##########################
##########################
##########################
##########################
#C MMVAE
##########################
##########################
##########################
##########################

def CMMVAE(dataset1, dataset2, target, n_runs, labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)

        # Reset the index if needed
        y1 = y1.reset_index(drop=True)
        y2 = y2.reset_index(drop=True)

        from sklearn.model_selection import train_test_split

        # Define your datasets
        X_datasets = [X1, X2]
        y_datasets = [y1, y2]

        # Dictionaries to store the train and test sets with indices starting from 1
        XTrain = {}
        XTest = {}
        YTrain = {}
        YTest = {}

        # Loop through each dataset and split into train and test sets
        for i in range(len(X_datasets)):
            y_labels = np.argmax(y_datasets[i], axis=1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_datasets[i],
                y_datasets[i],
                test_size=0.3,
                stratify=y_labels,
                random_state=seed
            )

            # Store the results in dictionaries, using i+1 as the key
            # Store the results in dictionaries using i+1 as the key to match numbering
            XTrain[f'XTrain_{i+1}'] = X_train
            XTest[f'XTest_{i+1}'] = X_test
            YTrain[f'YTrain_{i+1}'] = y_train
            YTest[f'YTest_{i+1}'] = y_test

        for i in range(1, len(XTrain) + 1):
            globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
            globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
            globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
            globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary



        # Loop to process datasets 1 and 2
        for i in range(1, 3):


            # Convert to matrix
            globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
            globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

            # One hot encode Y and convert to matrix
            globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
            globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

            # Get number of features
            globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation, name='encoder_input_2')(x_ini_2)
            xe_2 = Dropout(k, name='encoder_dropout_2')(xe_2)


            ############################

            ##############################

            CL_1 = Concatenate(axis=-1)([xe_1, xe_2])
            CL_2 = Concatenate(axis=-1)([xe_2, xe_1])


            CL_E_1 = Dense(num_neurons, activation=f_activation)(CL_1)
            CL_E_1 = Dropout(k)(CL_E_1)
            CL_E_2 = Dense(num_neurons, activation=f_activation)(CL_2)
            CL_E_2 = Dropout(k)(CL_E_2)
            ##############################

            # Concatenate Step 2

            CL_F = Concatenate(axis=-1)([CL_E_1, CL_E_2, labels])

            xe = Dense(num_neurons, activation=f_activation)(CL_F)
            xe = Dropout(k)(xe)

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)



            xd2 = Concatenate(axis=1)([xd1, labels])

            xd3 = Dense(num_neurons, activation=f_activation)(xd2)
            xd4 = Dropout(k)(xd3)

            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation, name='decoder_output_2')(xd4)
            xd_ds2 = Dropout(k, name='decoder_dropout_2')(xd_ds2)




            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)



            return Model(inputs=[x_ini_1, x_ini_2, labels], outputs=[xhat_1,xhat_2, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, xhat_2, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)

            return KLD + BCE_1 + BCE_2, KLD, BCE_1, BCE_2

        for i in range(1, 3):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons



        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop

        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1
        # Custom training loop

        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, xhat_2, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2 = cvae_loss([batch_xs1, batch_xs1_2], [xhat_1, xhat_2, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))



        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2

        _,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2, YTrainCLMHE_1])
        _,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest['YTest_1']
        y_train = YTrain['YTrain_1']
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model
        if len(y_test.shape) > 1 and y_test.shape[1] > 1:
            y_test = np.argmax(y_test, axis=1)

        if len(y_pred_RF.shape) > 1 and y_pred_RF.shape[1] > 1:
            y_pred_RF = np.argmax(y_pred_RF, axis=1)
        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test)

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        from sklearn.linear_model import LogisticRegression


        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test, y_pred_LR)

        accuracy_LR = accuracy_score(y_test, y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test, y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from CMMVAE  Architecture"
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results

#############################
#############################
#############################
#############################
#Integraing 3 datasets using extended CXVAE
#############################
#############################
#############################
#############################


def EXTCXVAE(dataset1, dataset2, dataset3, target, n_runs, labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        X3 = pd.read_csv(dataset3)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)
        y3 = pd.read_csv(target)

        # Reset the index if needed
        y1 = y1.reset_index(drop=True)
        y2 = y2.reset_index(drop=True)
        y3 = y3.reset_index(drop=True)

        from sklearn.model_selection import train_test_split

        # Define your datasets
        X_datasets = [X1, X2, X3]
        y_datasets = [y1, y2, y3]

        # Dictionaries to store the train and test sets with indices starting from 1
        XTrain = {}
        XTest = {}
        YTrain = {}
        YTest = {}

        # Loop through each dataset and split into train and test sets
        for i in range(len(X_datasets)):
            y_labels = np.argmax(y_datasets[i], axis=1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_datasets[i],
                y_datasets[i],
                test_size=0.3,
                stratify=y_labels,
                random_state=seed
            )

            # Store the results in dictionaries, using i+1 as the key
            # Store the results in dictionaries using i+1 as the key to match numbering
            XTrain[f'XTrain_{i+1}'] = X_train
            XTest[f'XTest_{i+1}'] = X_test
            YTrain[f'YTrain_{i+1}'] = y_train
            YTest[f'YTest_{i+1}'] = y_test

        for i in range(1, len(XTrain) + 1):
            globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
            globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
            globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
            globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary



        # Loop to process datasets 1 and 2
        for i in range(1, 4):


            # Convert to matrix
            globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
            globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

            # One hot encode Y and convert to matrix
            globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
            globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

            # Get number of features
            globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            x_ini_3 = Input(shape=(input_dim_3,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation)(x_ini_1)  # fully-connected layer
            xe_1 = Dropout(k)(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation)(x_ini_2)  # fully-connected layer
            xe_2 = Dropout(k)(xe_2)
            # Dataset 3
            xe_3 = Dense(num_neurons, activation=f_activation)(x_ini_2)  # fully-connected layer
            xe_3 = Dropout(k)(xe_3)

            ############################

            ##############################

            # Integration for encoder
            xe = Concatenate(axis=-1)([xe_1, xe_2, xe_3])

            ##############################

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            xe = Concatenate(axis=1)([xe, labels])

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)



            xd2 = Concatenate(axis=1)([xd1, labels])

            xd3 = Dense(num_neurons, activation=f_activation)(xd2)
            xd4 = Dropout(k)(xd3)

            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds1 = Dropout(k)(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds2 = Dropout(k)(xd_ds2)
            xd_ds3 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds3 = Dropout(k)(xd_ds3)



            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)
            xhat_3 = Dense(output_dim_3, activation='sigmoid')(xd_ds3)

            return Model(inputs=[x_ini_1, x_ini_2, x_ini_3, labels], outputs=[xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma])


        # Define the custom loss function
        def vae_loss(x_true, x_pred):
            xhat_1, xhat_2, xhat_3, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2, x_true_3 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)
            x_true_3 = tf.cast(x_true_3, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            E_log_px_given_z_DS3 = tf.reduce_sum(tf.square(x_true_3 - xhat_3), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)
            BCE_3 = tf.reduce_sum(E_log_px_given_z_DS3)

            return KLD + BCE_1 + BCE_2 + BCE_3, KLD, BCE_1, BCE_2, BCE_3

        for i in range(1, 4):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]

        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons



        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop

        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': [], 'bce_3': []}


        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtraining_3 = XTrainM_3
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        xtesting_3 = XTestM_3
        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1
        # Custom training loop

        for epoch in range(n_epochs):


            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()


            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]

                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]

                batch_xs_3 = xtraining_3[step * batch_size: (step + 1) * batch_size]
                batch_xs1_3 = xtraining_3_3[step * batch_size: (step + 1) * batch_size]

                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_xs_3, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2, BCE_3 = vae_loss([batch_xs1, batch_xs1_2, batch_xs1_3], [xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))




        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtraining_3 = XTrainM_3
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        xtesting_3 = XTestM_3

                # Extract latent space
        _,_,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2,xtraining_3, YTrainCLMHE_1])
        _,_,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, xtesting_3, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)

        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df



        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        xtesting_3 = XTestM_3

        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################


        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest['YTest_1']
        y_train = YTrain['YTrain_1']
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model
        if len(y_test.shape) > 1 and y_test.shape[1] > 1:
            y_test = np.argmax(y_test, axis=1)

        if len(y_pred_RF.shape) > 1 and y_pred_RF.shape[1] > 1:
            y_pred_RF = np.argmax(y_pred_RF, axis=1)
        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test)

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        from sklearn.linear_model import LogisticRegression


        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test, y_pred_LR)

        accuracy_LR = accuracy_score(y_test, y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test, y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)


    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)


    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)


    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from Extened CXVAE Architecture"
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results


#############################
#############################
#############################
#############################
#Integraing 3 datasets using extended XVAE
# Without labels
#############################
#############################
#############################
#############################
def EXTXVAE(dataset1, dataset2, dataset3, target, n_runs, labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        X3 = pd.read_csv(dataset3)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)
        y3 = pd.read_csv(target)

        # Reset the index if needed
        y1 = y1.reset_index(drop=True)
        y2 = y2.reset_index(drop=True)
        y3 = y3.reset_index(drop=True)

        from sklearn.model_selection import train_test_split

        # Define your datasets
        X_datasets = [X1, X2, X3]
        y_datasets = [y1, y2, y3]

        # Dictionaries to store the train and test sets with indices starting from 1
        XTrain = {}
        XTest = {}
        YTrain = {}
        YTest = {}

        # Loop through each dataset and split into train and test sets
        for i in range(len(X_datasets)):
            y_labels = np.argmax(y_datasets[i], axis=1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_datasets[i],
                y_datasets[i],
                test_size=0.3,
                stratify=y_labels,
                random_state=seed
            )

            # Store the results in dictionaries, using i+1 as the key
            # Store the results in dictionaries using i+1 as the key to match numbering
            XTrain[f'XTrain_{i+1}'] = X_train
            XTest[f'XTest_{i+1}'] = X_test
            YTrain[f'YTrain_{i+1}'] = y_train
            YTest[f'YTest_{i+1}'] = y_test

        for i in range(1, len(XTrain) + 1):
            globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
            globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
            globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
            globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary



        # Loop to process datasets 1 and 2
        for i in range(1, 4):


            # Convert to matrix
            globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
            globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

            # One hot encode Y and convert to matrix
            globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
            globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

            # Get number of features
            globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num

        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            x_ini_3 = Input(shape=(input_dim_3,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation)(x_ini_1)  # fully-connected layer
            xe_1 = Dropout(k)(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation)(x_ini_2)  # fully-connected layer
            xe_2 = Dropout(k)(xe_2)
            # Dataset 3
            xe_3 = Dense(num_neurons, activation=f_activation)(x_ini_2)  # fully-connected layer
            xe_3 = Dropout(k)(xe_3)

            ############################

            ##############################

            # Integration for encoder
            xe = Concatenate(axis=-1)([xe_1, xe_2, xe_3])

            ##############################

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            #xe = Concatenate(axis=1)([xe, labels])

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)



            #xd2 = Concatenate(axis=1)([xd1, labels])

            xd3 = Dense(num_neurons, activation=f_activation)(xd1)
            xd4 = Dropout(k)(xd3)
            #xd3 = Dense(64, activation=f_activation)(xd3)
            #xd4 = Dropout(k)(xd3)

            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds1 = Dropout(k)(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds2 = Dropout(k)(xd_ds2)
            xd_ds3 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds3 = Dropout(k)(xd_ds3)



            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)
            xhat_3 = Dense(output_dim_3, activation='sigmoid')(xd_ds3)

            return Model(inputs=[x_ini_1, x_ini_2, x_ini_3, labels], outputs=[xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma])

        # Define the custom loss function
        def vae_loss(x_true, x_pred):
            xhat_1, xhat_2, xhat_3, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2, x_true_3 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)
            x_true_3 = tf.cast(x_true_3, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            E_log_px_given_z_DS3 = tf.reduce_sum(tf.square(x_true_3 - xhat_3), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)
            BCE_3 = tf.reduce_sum(E_log_px_given_z_DS3)

            return KLD + BCE_1 + BCE_2 + BCE_3, KLD, BCE_1, BCE_2, BCE_3

        for i in range(1, 4):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]

        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons



        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop

        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': [], 'bce_3': []}


        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtraining_3 = XTrainM_3
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        xtesting_3 = XTestM_3
        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1
        # Custom training loop

        for epoch in range(n_epochs):


            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()


            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]

                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]

                batch_xs_3 = xtraining_3[step * batch_size: (step + 1) * batch_size]
                batch_xs1_3 = xtraining_3_3[step * batch_size: (step + 1) * batch_size]

                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_xs_3, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2, BCE_3 = vae_loss([batch_xs1, batch_xs1_2, batch_xs1_3], [xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))




        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtraining_3 = XTrainM_3
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        xtesting_3 = XTestM_3

                # Extract latent space
        _,_,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2,xtraining_3, YTrainCLMHE_1])
        _,_,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, xtesting_3, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)

        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df



        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        xtesting_3 = XTestM_3

        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################


        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest['YTest_1']
        y_train = YTrain['YTrain_1']
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model
        if len(y_test.shape) > 1 and y_test.shape[1] > 1:
            y_test = np.argmax(y_test, axis=1)

        if len(y_pred_RF.shape) > 1 and y_pred_RF.shape[1] > 1:
            y_pred_RF = np.argmax(y_pred_RF, axis=1)
        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test)

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        from sklearn.linear_model import LogisticRegression


        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test, y_pred_LR)

        accuracy_LR = accuracy_score(y_test, y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test, y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)


    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)


    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)


    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from Extened XVAE Architecture without labels"
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results



##########################
##########################
##########################
##########################
# XVAE
##########################
##########################
##########################
##########################

def XVAE(dataset1, dataset2, target, n_runs, labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)

        # Reset the index if needed
        y1 = y1.reset_index(drop=True)
        y2 = y2.reset_index(drop=True)

        from sklearn.model_selection import train_test_split

        # Define your datasets
        X_datasets = [X1, X2]
        y_datasets = [y1, y2]

        # Dictionaries to store the train and test sets with indices starting from 1
        XTrain = {}
        XTest = {}
        YTrain = {}
        YTest = {}

        # Loop through each dataset and split into train and test sets
        for i in range(len(X_datasets)):
            y_labels = np.argmax(y_datasets[i], axis=1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_datasets[i],
                y_datasets[i],
                test_size=0.3,
                stratify=y_labels,
                random_state=seed
            )

            # Store the results in dictionaries, using i+1 as the key
            # Store the results in dictionaries using i+1 as the key to match numbering
            XTrain[f'XTrain_{i+1}'] = X_train
            XTest[f'XTest_{i+1}'] = X_test
            YTrain[f'YTrain_{i+1}'] = y_train
            YTest[f'YTest_{i+1}'] = y_test

        for i in range(1, len(XTrain) + 1):
            globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
            globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
            globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
            globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary



        # Loop to process datasets 1 and 2
        for i in range(1, 3):


            # Convert to matrix
            globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
            globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

            # One hot encode Y and convert to matrix
            globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
            globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

            # Get number of features
            globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def VAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation, name='encoder_input_2')(x_ini_2)
            xe_2 = Dropout(k, name='encoder_dropout_2')(xe_2)


            ############################

            ##############################

            # Integration for encoder
            xe = Concatenate(axis=-1)([xe_1, xe_2])

            ##############################

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)


            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)


            xd3 = Dense(num_neurons, activation=f_activation)(xd1)
            xd4 = Dropout(k)(xd3)


            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation, name='decoder_output_2')(xd4)
            xd_ds2 = Dropout(k, name='decoder_dropout_2')(xd_ds2)




            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)



            return Model(inputs=[x_ini_1, x_ini_2, labels], outputs=[xhat_1,xhat_2, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, xhat_2, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)

            return KLD + BCE_1 + BCE_2, KLD, BCE_1, BCE_2

        for i in range(1, 3):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons



        # Configure the model
        model = VAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop

        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1
        # Custom training loop
        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, xhat_2, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2 = cvae_loss([batch_xs1, batch_xs1_2], [xhat_1, xhat_2, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2

        _,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2, YTrainCLMHE_1])
        _,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest['YTest_1']
        y_train = YTrain['YTrain_1']
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model
        if len(y_test.shape) > 1 and y_test.shape[1] > 1:
            y_test = np.argmax(y_test, axis=1)

        if len(y_pred_RF.shape) > 1 and y_pred_RF.shape[1] > 1:
            y_pred_RF = np.argmax(y_pred_RF, axis=1)
        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test)

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        from sklearn.linear_model import LogisticRegression


        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test, y_pred_LR)

        accuracy_LR = accuracy_score(y_test, y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test, y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from XVAE(No labels) Architecture"
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results



##########################
##########################
##########################
##########################
# MMVAE
##########################
##########################
##########################
##########################
def MMVAE(dataset1, dataset2, target, n_runs, labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)

        # Reset the index if needed
        y1 = y1.reset_index(drop=True)
        y2 = y2.reset_index(drop=True)

        from sklearn.model_selection import train_test_split

        # Define your datasets
        X_datasets = [X1, X2]
        y_datasets = [y1, y2]

        # Dictionaries to store the train and test sets with indices starting from 1
        XTrain = {}
        XTest = {}
        YTrain = {}
        YTest = {}

        # Loop through each dataset and split into train and test sets
        for i in range(len(X_datasets)):
            y_labels = np.argmax(y_datasets[i], axis=1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_datasets[i],
                y_datasets[i],
                test_size=0.3,
                stratify=y_labels,
                random_state=seed
            )

            # Store the results in dictionaries, using i+1 as the key
            # Store the results in dictionaries using i+1 as the key to match numbering
            XTrain[f'XTrain_{i+1}'] = X_train
            XTest[f'XTest_{i+1}'] = X_test
            YTrain[f'YTrain_{i+1}'] = y_train
            YTest[f'YTest_{i+1}'] = y_test

        for i in range(1, len(XTrain) + 1):
            globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
            globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
            globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
            globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary



        # Loop to process datasets 1 and 2
        for i in range(1, 3):


            # Convert to matrix
            globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
            globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

            # One hot encode Y and convert to matrix
            globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
            globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

            # Get number of features
            globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def VAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation, name='encoder_input_2')(x_ini_2)
            xe_2 = Dropout(k, name='encoder_dropout_2')(xe_2)


            ############################

            ##############################

            CL_1 = Concatenate(axis=-1)([xe_1, xe_2])
            CL_2 = Concatenate(axis=-1)([xe_2, xe_1])


            CL_E_1 = Dense(num_neurons, activation=f_activation)(CL_1)
            CL_E_1 = Dropout(k)(CL_E_1)
            CL_E_2 = Dense(num_neurons, activation=f_activation)(CL_2)
            CL_E_2 = Dropout(k)(CL_E_2)
            ##############################

            # Concatenate Step 2

            CL_F = Concatenate(axis=-1)([CL_E_1, CL_E_2])

            xe = Dense(num_neurons, activation=f_activation)(CL_F)
            xe = Dropout(k)(xe)

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)


            xd3 = Dense(num_neurons, activation=f_activation)(xd1)
            xd4 = Dropout(k)(xd3)

            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation, name='decoder_output_2')(xd4)
            xd_ds2 = Dropout(k, name='decoder_dropout_2')(xd_ds2)




            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)

            return Model(inputs=[x_ini_1, x_ini_2, labels], outputs=[xhat_1,xhat_2, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, xhat_2, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)

            return KLD + BCE_1 + BCE_2, KLD, BCE_1, BCE_2

        for i in range(1, 3):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons



        # Configure the model
        model = VAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop

        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1
        # Custom training loop
        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, xhat_2, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2 = cvae_loss([batch_xs1, batch_xs1_2], [xhat_1, xhat_2, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2

        _,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2, YTrainCLMHE_1])
        _,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest['YTest_1']
        y_train = YTrain['YTrain_1']
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model
        if len(y_test.shape) > 1 and y_test.shape[1] > 1:
            y_test = np.argmax(y_test, axis=1)

        if len(y_pred_RF.shape) > 1 and y_pred_RF.shape[1] > 1:
            y_pred_RF = np.argmax(y_pred_RF, axis=1)
        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test)

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        from sklearn.linear_model import LogisticRegression


        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test, y_pred_LR)

        accuracy_LR = accuracy_score(y_test, y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test, y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from MMVAE(No labels)  Architecture"
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results

##########################
##########################
##########################
##########################
#C XVAE for single dataset
##########################
##########################
##########################
##########################

def CXVAESD(dataset, target,n_runs, labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset)
        y1 = pd.read_csv(target)

        from sklearn.model_selection import train_test_split

        # Define your datasets
        X_datasets = [X1]
        y_datasets = [y1]

        # Dictionaries to store the train and test sets with indices starting from 1
        XTrain = {}
        XTest = {}
        YTrain = {}
        YTest = {}
        for i in range(len(X_datasets)):
            y_labels = np.argmax(y_datasets[i], axis=1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_datasets[i],
                y_datasets[i],
                test_size=0.3,
                stratify=y_labels,
                random_state=seed
            )

            # Store the results in dictionaries, using i+1 as the key
            # Store the results in dictionaries using i+1 as the key to match numbering
            XTrain[f'XTrain_{i+1}'] = X_train
            XTest[f'XTest_{i+1}'] = X_test
            YTrain[f'YTrain_{i+1}'] = y_train
            YTest[f'YTest_{i+1}'] = y_test

        for i in range(1, len(XTrain) + 1):
            globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
            globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
            globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
            globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


        # Loop to process datasets 1 and 2
        for i in range(1, 2):

            # Convert to matrix
            globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
            globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

            # One hot encode Y and convert to matrix
            globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
            globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

            # Get number of features
            globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]


        YTrainCLMHE_1 = YTrain_1
        YTestCLMHE_1 = YTest_1
        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            #x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)


            ############################

            ##############################


            xe = Concatenate(axis=1)([xe_1, labels])

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)



            xd2 = Concatenate(axis=1)([xd1, labels])

            xd3 = Dense(num_neurons, activation=f_activation)(xd2)
            xd4 = Dropout(k)(xd3)


            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)

            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)




            return Model(inputs=[x_ini_1, labels], outputs=[xhat_1, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, z_mu, z_log_sigma = x_pred
            x_true_1 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)

            return KLD + BCE_1, KLD, BCE_1

        for i in range(1, 2):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons


        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop
        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}
        xtraining_1 = XTrainM_1
        xtesting_1 = XTestM_1


        # Custom training loop

        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, z_mu, z_log_sigma = model([batch_xs, batch_labels], training=True)
                    loss_value, KLD, BCE_1= cvae_loss([batch_xs1], [xhat_1, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))



        xtraining_1 = XTrainM_1
        xtesting_1 = XTestM_1


        _, z_mu_train, z_log_sigma_train = model.predict([xtraining_1, YTrainCLMHE_1])
        _, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)




        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df





        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest['YTest_1']
        y_train = YTrain['YTrain_1']
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model
        if len(y_test.shape) > 1 and y_test.shape[1] > 1:
            y_test = np.argmax(y_test, axis=1)

        if len(y_pred_RF.shape) > 1 and y_pred_RF.shape[1] > 1:
            y_pred_RF = np.argmax(y_pred_RF, axis=1)
        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test)

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        from sklearn.linear_model import LogisticRegression


        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test, y_pred_LR)

        accuracy_LR = accuracy_score(y_test, y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test, y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from CVAE Architecture for single dataset"
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results


##########################
##########################
##########################
##########################
#Concatenation
##########################
##########################
##########################
##########################


def CONCAT(dataset1, dataset2, target,n_runs,labels_dim):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)

        # Reset the index if needed
        y1 = y1.reset_index(drop=True)


        from sklearn.model_selection import train_test_split

        # Define your datasets
        X_Concat = pd.concat([X1, X2], axis=1)
        X_datasets = [X_Concat]
        y_datasets = [y1]

        # Dictionaries to store the train and test sets with indices starting from 1
        XTrain = {}
        XTest = {}
        YTrain = {}
        YTest = {}

        # Loop through each dataset and split into train and test sets
        for i in range(len(X_datasets)):
            y_labels = np.argmax(y_datasets[i], axis=1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_datasets[i],
                y_datasets[i],
                test_size=0.3,
                stratify=y_labels,
                random_state=seed
            )

            # Store the results in dictionaries, using i+1 as the key
            # Store the results in dictionaries using i+1 as the key to match numbering
            XTrain[f'XTrain_{i+1}'] = X_train
            XTest[f'XTest_{i+1}'] = X_test
            YTrain[f'YTrain_{i+1}'] = y_train
            YTest[f'YTest_{i+1}'] = y_test

        for i in range(1, len(XTrain) + 1):
            globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
            globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
            globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
            globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary




        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest['YTest_1']
        y_train = YTrain['YTrain_1']
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model
        if len(y_test.shape) > 1 and y_test.shape[1] > 1:
            y_test = np.argmax(y_test, axis=1)

        if len(y_pred_RF.shape) > 1 and y_pred_RF.shape[1] > 1:
            y_pred_RF = np.argmax(y_pred_RF, axis=1)
        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test)

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        from sklearn.linear_model import LogisticRegression


        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test, y_pred_LR)

        accuracy_LR = accuracy_score(y_test, y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test, y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = XTest_1
        X_train = XTrain_1
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from Concatenating two datasets"
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results





##########################
##########################
##########################
##########################
#PCA Both
##########################
##########################
##########################
##########################


def PCA(dataset1, dataset2, target,n_runs,labels_dim):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)

        # Reset the index if needed
        y1 = y1.reset_index(drop=True)

        from sklearn.model_selection import train_test_split

        # Define your datasets
        X_Concat = pd.concat([X1, X2], axis=1)
        X_datasets = [X_Concat]
        y_datasets = [y1]
        XTrain = {}
        XTest = {}
        YTrain = {}
        YTest = {}
        # Loop through each dataset and split into train and test sets
        for i in range(len(X_datasets)):
            y_labels = np.argmax(y_datasets[i], axis=1)
            X_train, X_test, y_train, y_test = train_test_split(
                X_datasets[i],
                y_datasets[i],
                test_size=0.3,
                stratify=y_labels,
                random_state=seed
            )

            # Store the results in dictionaries, using i+1 as the key
            # Store the results in dictionaries using i+1 as the key to match numbering
            XTrain[f'XTrain_{i+1}'] = X_train
            XTest[f'XTest_{i+1}'] = X_test
            YTrain[f'YTrain_{i+1}'] = y_train
            YTest[f'YTest_{i+1}'] = y_test

        for i in range(1, len(XTrain) + 1):
            globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
            globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
            globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
            globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


        from sklearn.decomposition import PCA


        # Apply PCA on the training set
        pca = PCA(n_components=0.95)  # Retain 95% of variance
        X_train_pca = pca.fit_transform(XTrain_1)
        X_test_pca = pca.transform(XTest_1)

        train_latent_space = X_train_pca
        test_latent_space = X_test_pca


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest['YTest_1']
        y_train = YTrain['YTrain_1']
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model
        if len(y_test.shape) > 1 and y_test.shape[1] > 1:
            y_test = np.argmax(y_test, axis=1)

        if len(y_pred_RF.shape) > 1 and y_pred_RF.shape[1] > 1:
            y_pred_RF = np.argmax(y_pred_RF, axis=1)
        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test)

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)
        from sklearn.linear_model import LogisticRegression


        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test, y_pred_LR)

        accuracy_LR = accuracy_score(y_test, y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test, y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_train = np.argmax(YTrain['YTrain_1'].values, axis=1)
        y_test = np.argmax(YTest['YTest_1'].values, axis=1)

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from PCA"
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results