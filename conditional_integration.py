# -*- coding: utf-8 -*-
"""Conditional Integration

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WMFKL46nzn_g0MmkrEEyJsudBFKVpSEM
"""

##########################
##########################
##########################
##########################
#C XVAE Type 1
##########################
##########################
##########################
##########################

def CXVAET1(dataset1, dataset2, target, n_runs, input_type,labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)


        if input_type == 'PAM50':
            indices_to_remove = y1[y1['Pam50Subtype'] == '?'].index

            # Remove corresponding data from X1, y1, y2, and X2
            X1 = X1.drop(indices_to_remove)
            y1 = y1.drop(indices_to_remove)
            X2 = X2.drop(indices_to_remove)
            y2 = y2.drop(indices_to_remove)

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2]
            y_datasets = [y1, y2]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['Pam50Subtype'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


            label_map = {0: 'class1', 1: 'class2', 2: 'class3'}

            # Loop to process datasets 1 and 2
            for i in range(1, 3):


                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer


            # Loop through datasets 1 and 2
            for i in range(1, 3):
                # LabelBinarizer for the one-hot encoding
                lb = LabelBinarizer()
                lb.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lb.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lb.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For 5-label encoding using LabelBinarizer
                lb5 = LabelBinarizer()
                lb5.fit(globals()[f'YTrainCLM_{i}'])

                globals()[f'YTrainCLMHE_{i}'] = lb5.transform(globals()[f'YTrainCLM_{i}'])
                globals()[f'YTestCLMHE_{i}'] = lb5.transform(globals()[f'YTestCLM_{i}'])  # This may change YTest

                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()



            # Dictionary to map numerical labels to string labels
            label_mapping = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 3):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)

            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values


            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)

            YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTestCLM_2)
            #one-hot-encoded
            YTestCLMHE_2 = lblNW.transform(YTestCLM_2)

        elif input_type == 'Cathgen':
            scaler = MinMaxScaler()
            X1 = pd.DataFrame(scaler.fit_transform(X1), columns=X1.columns)
            X2 = pd.DataFrame(scaler.fit_transform(X2), columns=X2.columns)
            min_samples = min(X1.shape[0], X2.shape[0])
            X1 = X1.iloc[:min_samples]  # Truncate to the minimum number of samples
            X2 = X2.iloc[:min_samples]  # Truncate to the minimum number of samples
            y1 = y1.iloc[:min_samples]  # Truncate to the minimum number of samples
            y2 = y2.iloc[:min_samples]  # Truncate to the minimum number of samples

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2]
            y_datasets = [y1, y2]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['cadStatus'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary

            label_map = {0: 'class1', 1: 'class2'}
            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Replace the labels using the mapping
                globals()[f'YTest_{i}'] = globals()[f'YTest_{i}'].replace(label_map)
                globals()[f'YTrain_{i}'] = globals()[f'YTrain_{i}'].replace(label_map)

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'class1': 'Class1','class2': 'Class2'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values
                #LabelToGroup = {str(k): v for k, v in LabelToGroup.items()}

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer



            # Loop through datasets 1 and 2
            for i in range(1, 3):
                # LabelBinarizer for the one-hot encoding
                lbl = LabelBinarizer()
                lbl.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lbl.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lbl.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For label encoding using LabelBinarizer
                lblNW = LabelBinarizer()
                lblNW.fit(YTrainCLM_1)
                #one-hot-encoded
                YTrainCLMHE_1 = lblNW.transform(YTrainCLM_1)

                #################
                # Unique to 2 labels

                # Initialize OneHotEncoder
                ohe = OneHotEncoder(sparse_output=False)
                # Fit and transform the data to one-hot encoding
                YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
                #################
                lb5_2 = LabelBinarizer()
                lb5_2.fit(YTrainCLM_2)

                YTrainCLMHE_2 = lb5_2.transform(YTrainCLM_2)
                YTestCLMHE_2 = lb5_2.transform(YTestCLM_2) # this transformation changes YTest


                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()

          # Dictionary to map numerical labels to string labels
            label_mapping = {'class1': 'Class1', 'class2': 'Class2'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 3):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)



            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values


            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)
        else:
            raise ValueError("Invalid input type.")

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation, name='encoder_input_2')(x_ini_2)
            xe_2 = Dropout(k, name='encoder_dropout_2')(xe_2)


            ############################

            ##############################

            # Integration for encoder
            xe = Concatenate(axis=-1)([xe_1, xe_2])

            ##############################

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            xe = Concatenate(axis=1)([xe, labels])

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)



            xd2 = Concatenate(axis=1)([xd1, labels])

            xd3 = Dense(num_neurons, activation=f_activation)(xd2)
            xd4 = Dropout(k)(xd3)


            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation, name='decoder_output_2')(xd4)
            xd_ds2 = Dropout(k, name='decoder_dropout_2')(xd_ds2)




            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)



            return Model(inputs=[x_ini_1, x_ini_2, labels], outputs=[xhat_1,xhat_2, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, xhat_2, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)

            return KLD + BCE_1 + BCE_2, KLD, BCE_1, BCE_2

        for i in range(1, 3):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons



        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop

        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        # Custom training loop

        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, xhat_2, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2 = cvae_loss([batch_xs1, batch_xs1_2], [xhat_1, xhat_2, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))



        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2

        _,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2, YTrainCLMHE_1])
        _,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)
           # We save the latent space for the specific method because the same name was used for the other methods
        z_mu_train_df.to_csv('train_latent_space_CXVAET1.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space_CXVAET1.csv', index=False)


        YTrain = pd.DataFrame(YTrainM_1)
        YTrain.columns = ['class']
        YTrain.to_csv('YTrain_CLS.csv', index=False)

        # Save latent space to CSV files
        YTest = pd.DataFrame(YTestM_1)
        YTest.columns = ['class']
        YTest.to_csv('YTest_CLS.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model

        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train['class']) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test['class'])

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        from sklearn.linear_model import LogisticRegression

        unique_classes = y_train['class'].unique()

        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train['class']) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test['class'], y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test['class'], y_pred_LR)

        accuracy_LR = accuracy_score(y_test['class'], y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test['class'], y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from C XVAE Type 1 Architecture. "
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results

##########################
##########################
##########################
##########################
#C XVAE Type 2
##########################
##########################
##########################
##########################


def CXVAET2(dataset1, dataset2, target,n_runs, input_type,labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)

        if input_type == 'PAM50':
            indices_to_remove = y1[y1['Pam50Subtype'] == '?'].index

            # Remove corresponding data from X1, y1, y2, and X2
            X1 = X1.drop(indices_to_remove)
            y1 = y1.drop(indices_to_remove)
            X2 = X2.drop(indices_to_remove)
            y2 = y2.drop(indices_to_remove)

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2]
            y_datasets = [y1, y2]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['Pam50Subtype'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


            label_map = {0: 'class1', 1: 'class2', 2: 'class3'}

            # Loop to process datasets 1 and 2
            for i in range(1, 3):


                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer


            # Loop through datasets 1 and 2
            for i in range(1, 3):
                # LabelBinarizer for the one-hot encoding
                lb = LabelBinarizer()
                lb.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lb.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lb.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For 5-label encoding using LabelBinarizer
                lb5 = LabelBinarizer()
                lb5.fit(globals()[f'YTrainCLM_{i}'])

                globals()[f'YTrainCLMHE_{i}'] = lb5.transform(globals()[f'YTrainCLM_{i}'])
                globals()[f'YTestCLMHE_{i}'] = lb5.transform(globals()[f'YTestCLM_{i}'])  # This may change YTest

                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()



            # Dictionary to map numerical labels to string labels
            label_mapping = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 3):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)

            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)

            YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTestCLM_2)
            #one-hot-encoded
            YTestCLMHE_2 = lblNW.transform(YTestCLM_2)

        elif input_type == 'Cathgen':
            scaler = MinMaxScaler()
            X1 = pd.DataFrame(scaler.fit_transform(X1), columns=X1.columns)
            X2 = pd.DataFrame(scaler.fit_transform(X2), columns=X2.columns)
            min_samples = min(X1.shape[0], X2.shape[0])
            X1 = X1.iloc[:min_samples]  # Truncate to the minimum number of samples
            X2 = X2.iloc[:min_samples]  # Truncate to the minimum number of samples
            y1 = y1.iloc[:min_samples]  # Truncate to the minimum number of samples
            y2 = y2.iloc[:min_samples]  # Truncate to the minimum number of samples

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2]
            y_datasets = [y1, y2]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['cadStatus'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary

            label_map = {0: 'class1', 1: 'class2'}
            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Replace the labels using the mapping
                globals()[f'YTest_{i}'] = globals()[f'YTest_{i}'].replace(label_map)
                globals()[f'YTrain_{i}'] = globals()[f'YTrain_{i}'].replace(label_map)

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'class1': 'Class1','class2': 'Class2'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values
                #LabelToGroup = {str(k): v for k, v in LabelToGroup.items()}

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer



            # Loop through datasets 1 and 2
            for i in range(1, 3):
                # LabelBinarizer for the one-hot encoding
                lbl = LabelBinarizer()
                lbl.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lbl.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lbl.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For label encoding using LabelBinarizer
                lblNW = LabelBinarizer()
                lblNW.fit(YTrainCLM_1)
                #one-hot-encoded
                YTrainCLMHE_1 = lblNW.transform(YTrainCLM_1)

                #################
                # Unique to 2 labels

                # Initialize OneHotEncoder
                ohe = OneHotEncoder(sparse_output=False)
                # Fit and transform the data to one-hot encoding
                YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
                #################
                lb5_2 = LabelBinarizer()
                lb5_2.fit(YTrainCLM_2)

                YTrainCLMHE_2 = lb5_2.transform(YTrainCLM_2)
                YTestCLMHE_2 = lb5_2.transform(YTestCLM_2) # this transformation changes YTest


                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()

          # Dictionary to map numerical labels to string labels
            label_mapping = {'class1': 'Class1', 'class2': 'Class2'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 3):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)



            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)
        else:
            raise ValueError("Invalid input type.")



        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation, name='encoder_input_2')(x_ini_2)
            xe_2 = Dropout(k, name='encoder_dropout_2')(xe_2)


            ############################

            ##############################

            # Integration for encoder
            xe = Concatenate(axis=-1)([xe_1, xe_2, labels])

            ##############################

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)


            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################

            z = Concatenate(axis=1)([z, labels])
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)

            xd3 = Dense(num_neurons, activation=f_activation)(xd1)
            xd4 = Dropout(k)(xd3)


            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation, name='decoder_output_2')(xd4)
            xd_ds2 = Dropout(k, name='decoder_dropout_2')(xd_ds2)




            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)



            return Model(inputs=[x_ini_1, x_ini_2, labels], outputs=[xhat_1,xhat_2, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, xhat_2, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)

            return KLD + BCE_1 + BCE_2, KLD, BCE_1, BCE_2

        for i in range(1, 3):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars =  num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons


        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop
        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        # Custom training loop

        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, xhat_2, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2 = cvae_loss([batch_xs1, batch_xs1_2], [xhat_1, xhat_2, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))



        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2

        _,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2, YTrainCLMHE_1])
        _,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)
           # We save the latent space for the specific method because the same name was used for the other methods
        z_mu_train_df.to_csv('train_latent_space_CXVAET2.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space_CXVAET2.csv', index=False)


        YTrain = pd.DataFrame(YTrainM_1)
        YTrain.columns = ['class']
        YTrain.to_csv('YTrain_CLS.csv', index=False)

        # Save latent space to CSV files
        YTest = pd.DataFrame(YTestM_1)
        YTest.columns = ['class']
        YTest.to_csv('YTest_CLS.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model

        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train['class']) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test['class'])

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        from sklearn.linear_model import LogisticRegression

        unique_classes = y_train['class'].unique()

        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train['class']) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test['class'], y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test['class'], y_pred_LR)

        accuracy_LR = accuracy_score(y_test['class'], y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test['class'], y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)


    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)



    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)


    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)



    results = {
        "Results from C XVAE Type 2 Architecture. "
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results



##########################
##########################
##########################
##########################
#C MMVAE
##########################
##########################
##########################
##########################


def CMMVAE(dataset1, dataset2, target,n_runs, input_type,labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)

        if input_type == 'PAM50':
            indices_to_remove = y1[y1['Pam50Subtype'] == '?'].index

            # Remove corresponding data from X1, y1, y2, and X2
            X1 = X1.drop(indices_to_remove)
            y1 = y1.drop(indices_to_remove)
            X2 = X2.drop(indices_to_remove)
            y2 = y2.drop(indices_to_remove)

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2]
            y_datasets = [y1, y2]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['Pam50Subtype'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


            label_map = {0: 'class1', 1: 'class2', 2: 'class3'}

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer


            # Loop through datasets 1 and 2
            for i in range(1, 3):
                # LabelBinarizer for the one-hot encoding
                lb = LabelBinarizer()
                lb.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lb.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lb.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For 5-label encoding using LabelBinarizer
                lb5 = LabelBinarizer()
                lb5.fit(globals()[f'YTrainCLM_{i}'])

                globals()[f'YTrainCLMHE_{i}'] = lb5.transform(globals()[f'YTrainCLM_{i}'])
                globals()[f'YTestCLMHE_{i}'] = lb5.transform(globals()[f'YTestCLM_{i}'])  # This may change YTest

                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()



            # Dictionary to map numerical labels to string labels
            label_mapping = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 3):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)

            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)

            YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTestCLM_2)
            #one-hot-encoded
            YTestCLMHE_2 = lblNW.transform(YTestCLM_2)

        elif input_type == 'Cathgen':
            scaler = MinMaxScaler()
            X1 = pd.DataFrame(scaler.fit_transform(X1), columns=X1.columns)
            X2 = pd.DataFrame(scaler.fit_transform(X2), columns=X2.columns)
            min_samples = min(X1.shape[0], X2.shape[0])
            X1 = X1.iloc[:min_samples]  # Truncate to the minimum number of samples
            X2 = X2.iloc[:min_samples]  # Truncate to the minimum number of samples
            y1 = y1.iloc[:min_samples]  # Truncate to the minimum number of samples
            y2 = y2.iloc[:min_samples]  # Truncate to the minimum number of samples

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2]
            y_datasets = [y1, y2]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['cadStatus'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary

            label_map = {0: 'class1', 1: 'class2'}
            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Replace the labels using the mapping
                globals()[f'YTest_{i}'] = globals()[f'YTest_{i}'].replace(label_map)
                globals()[f'YTrain_{i}'] = globals()[f'YTrain_{i}'].replace(label_map)

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'class1': 'Class1','class2': 'Class2'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values
                #LabelToGroup = {str(k): v for k, v in LabelToGroup.items()}

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer



            # Loop through datasets 1 and 2
            for i in range(1, 3):
                # LabelBinarizer for the one-hot encoding
                lbl = LabelBinarizer()
                lbl.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lbl.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lbl.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For label encoding using LabelBinarizer
                lblNW = LabelBinarizer()
                lblNW.fit(YTrainCLM_1)
                #one-hot-encoded
                YTrainCLMHE_1 = lblNW.transform(YTrainCLM_1)

                #################
                # Unique to 2 labels

                # Initialize OneHotEncoder
                ohe = OneHotEncoder(sparse_output=False)
                # Fit and transform the data to one-hot encoding
                YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
                #################
                lb5_2 = LabelBinarizer()
                lb5_2.fit(YTrainCLM_2)

                YTrainCLMHE_2 = lb5_2.transform(YTrainCLM_2)
                YTestCLMHE_2 = lb5_2.transform(YTestCLM_2) # this transformation changes YTest



                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()

          # Dictionary to map numerical labels to string labels
            label_mapping = {'class1': 'Class1', 'class2': 'Class2'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 3):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)



            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)
        else:
            raise ValueError("Invalid input type.")

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation, name='encoder_input_2')(x_ini_2)
            xe_2 = Dropout(k, name='encoder_dropout_2')(xe_2)


            ############################

            ##############################

            CL_1 = Concatenate(axis=-1)([xe_1, xe_2])
            CL_2 = Concatenate(axis=-1)([xe_2, xe_1])


            CL_E_1 = Dense(num_neurons, activation=f_activation)(CL_1)
            CL_E_1 = Dropout(k)(CL_E_1)
            CL_E_2 = Dense(num_neurons, activation=f_activation)(CL_2)
            CL_E_2 = Dropout(k)(CL_E_2)
            ##############################

            # Concatenate Step 2

            CL_F = Concatenate(axis=-1)([CL_E_1, CL_E_2, labels])

            xe = Dense(num_neurons, activation=f_activation)(CL_F)
            xe = Dropout(k)(xe)

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)



            xd2 = Concatenate(axis=1)([xd1, labels])

            xd3 = Dense(num_neurons, activation=f_activation)(xd2)
            xd4 = Dropout(k)(xd3)

            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation, name='decoder_output_2')(xd4)
            xd_ds2 = Dropout(k, name='decoder_dropout_2')(xd_ds2)




            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)



            return Model(inputs=[x_ini_1, x_ini_2, labels], outputs=[xhat_1,xhat_2, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, xhat_2, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)

            return KLD + BCE_1 + BCE_2, KLD, BCE_1, BCE_2

        for i in range(1, 3):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons


        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop
        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        # Custom training loop

        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, xhat_2, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2 = cvae_loss([batch_xs1, batch_xs1_2], [xhat_1, xhat_2, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))



        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2

        _,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2, YTrainCLMHE_1])
        _,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)
        # We save the latent space for the specific method because the same name was used for the other methods
        z_mu_train_df.to_csv('train_latent_space_CMMVAE.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space_CMMVAE.csv', index=False)


        YTrain = pd.DataFrame(YTrainM_1)
        YTrain.columns = ['class']
        YTrain.to_csv('YTrain_CLS.csv', index=False)

        # Save latent space to CSV files
        YTest = pd.DataFrame(YTestM_1)
        YTest.columns = ['class']
        YTest.to_csv('YTest_CLS.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model

        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train['class']) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test['class'])

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        from sklearn.linear_model import LogisticRegression

        unique_classes = y_train['class'].unique()

        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train['class']) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test['class'], y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test['class'], y_pred_LR)

        accuracy_LR = accuracy_score(y_test['class'], y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test['class'], y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)



    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)


    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)



    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from C MM VAE Architecture. "
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results


#############################
#############################
#############################
#############################
#Integraing 3 datasets using extended CXVAE
#############################
#############################
#############################
#############################


def EXTCXVAE(dataset1, dataset2, dataset3, target, n_runs, input_type,labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        X3 = pd.read_csv(dataset3)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)
        y3 = pd.read_csv(target)


        if input_type == 'PAM50':
            indices_to_remove = y1[y1['Pam50Subtype'] == '?'].index

            # Remove corresponding data from X1, y1, y2, and X2
            X1 = X1.drop(indices_to_remove)
            y1 = y1.drop(indices_to_remove)
            X2 = X2.drop(indices_to_remove)
            y2 = y2.drop(indices_to_remove)
            X3 = X3.drop(indices_to_remove)
            y3 = y3.drop(indices_to_remove)

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)
            y3 = y3.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2,X3]
            y_datasets = [y1, y2,y3]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['Pam50Subtype'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


            label_map = {0: 'class1', 1: 'class2', 2: 'class3'}

            # Loop to process datasets 1 and 2
            for i in range(1, 4):

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 4):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer


            # Loop through datasets 1 and 2
            for i in range(1, 4):
                # LabelBinarizer for the one-hot encoding
                lb = LabelBinarizer()
                lb.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lb.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lb.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For 5-label encoding using LabelBinarizer
                lb5 = LabelBinarizer()
                lb5.fit(globals()[f'YTrainCLM_{i}'])

                globals()[f'YTrainCLMHE_{i}'] = lb5.transform(globals()[f'YTrainCLM_{i}'])
                globals()[f'YTestCLMHE_{i}'] = lb5.transform(globals()[f'YTestCLM_{i}'])  # This may change YTest

                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()



            # Dictionary to map numerical labels to string labels
            label_mapping = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 4):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)

            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)

            YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTestCLM_2)
            #one-hot-encoded
            YTestCLMHE_2 = lblNW.transform(YTestCLM_2)
        else:
            raise ValueError("Invalid input type.")

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            x_ini_3 = Input(shape=(input_dim_3,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation)(x_ini_1)  # fully-connected layer
            xe_1 = Dropout(k)(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation)(x_ini_2)  # fully-connected layer
            xe_2 = Dropout(k)(xe_2)
            # Dataset 3
            xe_3 = Dense(num_neurons, activation=f_activation)(x_ini_2)  # fully-connected layer
            xe_3 = Dropout(k)(xe_3)

            ############################

            ##############################

            # Integration for encoder
            xe = Concatenate(axis=-1)([xe_1, xe_2, xe_3])

            ##############################

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            xe = Concatenate(axis=1)([xe, labels])

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)



            xd2 = Concatenate(axis=1)([xd1, labels])

            xd3 = Dense(num_neurons, activation=f_activation)(xd2)
            xd4 = Dropout(k)(xd3)

            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds1 = Dropout(k)(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds2 = Dropout(k)(xd_ds2)
            xd_ds3 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds3 = Dropout(k)(xd_ds3)



            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)
            xhat_3 = Dense(output_dim_3, activation='sigmoid')(xd_ds3)

            return Model(inputs=[x_ini_1, x_ini_2, x_ini_3, labels], outputs=[xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma])


        # Define the custom loss function
        def vae_loss(x_true, x_pred):
            xhat_1, xhat_2, xhat_3, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2, x_true_3 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)
            x_true_3 = tf.cast(x_true_3, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            E_log_px_given_z_DS3 = tf.reduce_sum(tf.square(x_true_3 - xhat_3), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)
            BCE_3 = tf.reduce_sum(E_log_px_given_z_DS3)

            return KLD + BCE_1 + BCE_2 + BCE_3, KLD, BCE_1, BCE_2, BCE_3

        for i in range(1, 4):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]

        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons



        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop

        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': [], 'bce_3': []}


        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtraining_3 = XTrainM_3
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        xtesting_3 = XTestM_3

        # Custom training loop

        for epoch in range(n_epochs):


            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()


            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]

                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]

                batch_xs_3 = xtraining_3[step * batch_size: (step + 1) * batch_size]
                batch_xs1_3 = xtraining_3_3[step * batch_size: (step + 1) * batch_size]

                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_xs_3, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2, BCE_3 = vae_loss([batch_xs1, batch_xs1_2, batch_xs1_3], [xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))




                # Extract latent space
        _,_,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2,xtraining_3, YTrainCLMHE_1])
        _,_,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, xtesting_3, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)


        YTrain = pd.DataFrame(YTrainM_1)
        YTrain.columns = ['class']
        YTrain.to_csv('YTrain_CLS.csv', index=False)

        # Save latent space to CSV files
        YTest = pd.DataFrame(YTestM_1)
        YTest.columns = ['class']
        YTest.to_csv('YTest_CLS.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        xtesting_3 = XTestM_3

        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model

        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train['class']) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test['class'])

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        from sklearn.linear_model import LogisticRegression

        unique_classes = y_train['class'].unique()

        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train['class']) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test['class'], y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test['class'], y_pred_LR)

        accuracy_LR = accuracy_score(y_test['class'], y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test['class'], y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)


    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)


    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from Extened CXVAE Architecture. "
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results


#############################
#############################
#############################
#############################
#Integraing 3 datasets using extended XVAE
# Without labels
#############################
#############################
#############################
#############################


def EXTXVAE(dataset1, dataset2, dataset3, target, n_runs, input_type,labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        X3 = pd.read_csv(dataset3)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)
        y3 = pd.read_csv(target)


        if input_type == 'PAM50':
            indices_to_remove = y1[y1['Pam50Subtype'] == '?'].index

            # Remove corresponding data from X1, y1, y2, and X2
            X1 = X1.drop(indices_to_remove)
            y1 = y1.drop(indices_to_remove)
            X2 = X2.drop(indices_to_remove)
            y2 = y2.drop(indices_to_remove)
            X3 = X3.drop(indices_to_remove)
            y3 = y3.drop(indices_to_remove)

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)
            y3 = y3.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2,X3]
            y_datasets = [y1, y2,y3]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['Pam50Subtype'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


            label_map = {0: 'class1', 1: 'class2', 2: 'class3'}

            # Loop to process datasets 1 and 2
            for i in range(1, 4):

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 4):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer


            # Loop through datasets 1 and 2
            for i in range(1, 4):
                # LabelBinarizer for the one-hot encoding
                lb = LabelBinarizer()
                lb.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lb.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lb.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For 5-label encoding using LabelBinarizer
                lb5 = LabelBinarizer()
                lb5.fit(globals()[f'YTrainCLM_{i}'])

                globals()[f'YTrainCLMHE_{i}'] = lb5.transform(globals()[f'YTrainCLM_{i}'])
                globals()[f'YTestCLMHE_{i}'] = lb5.transform(globals()[f'YTestCLM_{i}'])  # This may change YTest

                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()



            # Dictionary to map numerical labels to string labels
            label_mapping = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 4):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)

            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)

            YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTestCLM_2)
            #one-hot-encoded
            YTestCLMHE_2 = lblNW.transform(YTestCLM_2)
        else:
            raise ValueError("Invalid input type.")

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            x_ini_3 = Input(shape=(input_dim_3,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation)(x_ini_1)  # fully-connected layer
            xe_1 = Dropout(k)(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation)(x_ini_2)  # fully-connected layer
            xe_2 = Dropout(k)(xe_2)
            # Dataset 3
            xe_3 = Dense(num_neurons, activation=f_activation)(x_ini_2)  # fully-connected layer
            xe_3 = Dropout(k)(xe_3)

            ############################

            ##############################

            # Integration for encoder
            xe = Concatenate(axis=-1)([xe_1, xe_2, xe_3])

            ##############################

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            #xe = Concatenate(axis=1)([xe, labels])

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)



            #xd2 = Concatenate(axis=1)([xd1, labels])

            xd3 = Dense(num_neurons, activation=f_activation)(xd1)
            xd4 = Dropout(k)(xd3)
            #xd3 = Dense(64, activation=f_activation)(xd3)
            #xd4 = Dropout(k)(xd3)

            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds1 = Dropout(k)(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds2 = Dropout(k)(xd_ds2)
            xd_ds3 = Dense(num_neurons, activation=f_activation)(xd4)
            xd_ds3 = Dropout(k)(xd_ds3)



            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)
            xhat_3 = Dense(output_dim_3, activation='sigmoid')(xd_ds3)

            return Model(inputs=[x_ini_1, x_ini_2, x_ini_3, labels], outputs=[xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma])


        # Define the custom loss function
        def vae_loss(x_true, x_pred):
            xhat_1, xhat_2, xhat_3, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2, x_true_3 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)
            x_true_3 = tf.cast(x_true_3, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            E_log_px_given_z_DS3 = tf.reduce_sum(tf.square(x_true_3 - xhat_3), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)
            BCE_3 = tf.reduce_sum(E_log_px_given_z_DS3)

            return KLD + BCE_1 + BCE_2 + BCE_3, KLD, BCE_1, BCE_2, BCE_3

        for i in range(1, 4):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]

        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons



        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop

        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': [], 'bce_3': []}


        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtraining_3 = XTrainM_3
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        xtesting_3 = XTestM_3

        # Custom training loop

        for epoch in range(n_epochs):


            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()


            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]

                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]

                batch_xs_3 = xtraining_3[step * batch_size: (step + 1) * batch_size]
                batch_xs1_3 = xtraining_3_3[step * batch_size: (step + 1) * batch_size]

                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_xs_3, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2, BCE_3 = vae_loss([batch_xs1, batch_xs1_2, batch_xs1_3], [xhat_1,xhat_2,xhat_3, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))




                # Extract latent space
        _,_,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2,xtraining_3, YTrainCLMHE_1])
        _,_,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, xtesting_3, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)
           # We save the latent space for the specific method because the same name was used for the other methods
        z_mu_train_df.to_csv('train_latent_space_ExtXVAE.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space_ExtXVAE.csv', index=False)


        YTrain = pd.DataFrame(YTrainM_1)
        YTrain.columns = ['class']
        YTrain.to_csv('YTrain_CLS.csv', index=False)

        # Save latent space to CSV files
        YTest = pd.DataFrame(YTestM_1)
        YTest.columns = ['class']
        YTest.to_csv('YTest_CLS.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        xtesting_3 = XTestM_3

        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model

        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train['class']) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test['class'])

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        from sklearn.linear_model import LogisticRegression

        unique_classes = y_train['class'].unique()

        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train['class']) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test['class'], y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test['class'], y_pred_LR)

        accuracy_LR = accuracy_score(y_test['class'], y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test['class'], y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)



    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)



    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)


    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)


    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from Extended XVAE(No label) Architecture. "
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results



##########################
##########################
##########################
##########################
# XVAE
##########################
##########################
##########################
##########################

def XVAE(dataset1, dataset2, target,n_runs, input_type,labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)
        if input_type == 'PAM50':
            indices_to_remove = y1[y1['Pam50Subtype'] == '?'].index

            # Remove corresponding data from X1, y1, y2, and X2
            X1 = X1.drop(indices_to_remove)
            y1 = y1.drop(indices_to_remove)
            X2 = X2.drop(indices_to_remove)
            y2 = y2.drop(indices_to_remove)

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2]
            y_datasets = [y1, y2]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['Pam50Subtype'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


            label_map = {0: 'class1', 1: 'class2', 2: 'class3'}

            # Loop to process datasets 1 and 2
            for i in range(1, 3):

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer


            # Loop through datasets 1 and 2
            for i in range(1, 3):
                # LabelBinarizer for the one-hot encoding
                lb = LabelBinarizer()
                lb.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lb.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lb.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For 5-label encoding using LabelBinarizer
                lb5 = LabelBinarizer()
                lb5.fit(globals()[f'YTrainCLM_{i}'])

                globals()[f'YTrainCLMHE_{i}'] = lb5.transform(globals()[f'YTrainCLM_{i}'])
                globals()[f'YTestCLMHE_{i}'] = lb5.transform(globals()[f'YTestCLM_{i}'])  # This may change YTest

                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()



            # Dictionary to map numerical labels to string labels
            label_mapping = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 3):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)

            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)

            YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTestCLM_2)
            #one-hot-encoded
            YTestCLMHE_2 = lblNW.transform(YTestCLM_2)

        elif input_type == 'Cathgen':
            scaler = MinMaxScaler()
            X1 = pd.DataFrame(scaler.fit_transform(X1), columns=X1.columns)
            X2 = pd.DataFrame(scaler.fit_transform(X2), columns=X2.columns)
            min_samples = min(X1.shape[0], X2.shape[0])
            X1 = X1.iloc[:min_samples]  # Truncate to the minimum number of samples
            X2 = X2.iloc[:min_samples]  # Truncate to the minimum number of samples
            y1 = y1.iloc[:min_samples]  # Truncate to the minimum number of samples
            y2 = y2.iloc[:min_samples]  # Truncate to the minimum number of samples

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2]
            y_datasets = [y1, y2]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['cadStatus'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary

            label_map = {0: 'class1', 1: 'class2'}
            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Replace the labels using the mapping
                globals()[f'YTest_{i}'] = globals()[f'YTest_{i}'].replace(label_map)
                globals()[f'YTrain_{i}'] = globals()[f'YTrain_{i}'].replace(label_map)

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'class1': 'Class1','class2': 'Class2'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values
                #LabelToGroup = {str(k): v for k, v in LabelToGroup.items()}

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer



            # Loop through datasets 1 and 2
            for i in range(1, 3):
                # LabelBinarizer for the one-hot encoding
                lbl = LabelBinarizer()
                lbl.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lbl.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lbl.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For label encoding using LabelBinarizer
                lblNW = LabelBinarizer()
                lblNW.fit(YTrainCLM_1)
                #one-hot-encoded
                YTrainCLMHE_1 = lblNW.transform(YTrainCLM_1)

                #################
                # Unique to 2 labels

                # Initialize OneHotEncoder
                ohe = OneHotEncoder(sparse_output=False)
                # Fit and transform the data to one-hot encoding
                YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
                #################
                lb5_2 = LabelBinarizer()
                lb5_2.fit(YTrainCLM_2)

                YTrainCLMHE_2 = lb5_2.transform(YTrainCLM_2)
                YTestCLMHE_2 = lb5_2.transform(YTestCLM_2) # this transformation changes YTest
                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()

          # Dictionary to map numerical labels to string labels
            label_mapping = {'class1': 'Class1', 'class2': 'Class2'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 3):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)



            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)
        else:
            raise ValueError("Invalid input type.")

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def XVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation, name='encoder_input_2')(x_ini_2)
            xe_2 = Dropout(k, name='encoder_dropout_2')(xe_2)


            ############################

            ##############################

            # Integration for encoder
            xe = Concatenate(axis=-1)([xe_1, xe_2])

            ##############################

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)


            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)


            xd3 = Dense(num_neurons, activation=f_activation)(xd1)
            xd4 = Dropout(k)(xd3)


            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation, name='decoder_output_2')(xd4)
            xd_ds2 = Dropout(k, name='decoder_dropout_2')(xd_ds2)




            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)



            return Model(inputs=[x_ini_1, x_ini_2, labels], outputs=[xhat_1,xhat_2, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, xhat_2, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)

            return KLD + BCE_1 + BCE_2, KLD, BCE_1, BCE_2

        for i in range(1, 3):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons


        # Configure the model
        model = XVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop
        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        # Custom training loop

        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, xhat_2, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2 = cvae_loss([batch_xs1, batch_xs1_2], [xhat_1, xhat_2, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))



        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2

        _,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2, YTrainCLMHE_1])
        _,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)
           # We save the latent space for the specific method because the same name was used for the other methods
        z_mu_train_df.to_csv('train_latent_space_XVAE.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space_XVAE.csv', index=False)


        YTrain = pd.DataFrame(YTrainM_1)
        YTrain.columns = ['class']
        YTrain.to_csv('YTrain_CLS.csv', index=False)

        # Save latent space to CSV files
        YTest = pd.DataFrame(YTestM_1)
        YTest.columns = ['class']
        YTest.to_csv('YTest_CLS.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model

        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train['class']) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test['class'])

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        from sklearn.linear_model import LogisticRegression

        unique_classes = y_train['class'].unique()

        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train['class']) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test['class'], y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test['class'], y_pred_LR)

        accuracy_LR = accuracy_score(y_test['class'], y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test['class'], y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)


    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)


    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)

    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)


    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)

    results = {
        "Results from XVAE Architecture. "
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results


##########################
##########################
##########################
##########################
# MMVAE
##########################
##########################
##########################
##########################


def MMVAE(dataset1, dataset2, target,n_runs, input_type,labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)
        if input_type == 'PAM50':
            indices_to_remove = y1[y1['Pam50Subtype'] == '?'].index

            # Remove corresponding data from X1, y1, y2, and X2
            X1 = X1.drop(indices_to_remove)
            y1 = y1.drop(indices_to_remove)
            X2 = X2.drop(indices_to_remove)
            y2 = y2.drop(indices_to_remove)

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2]
            y_datasets = [y1, y2]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['Pam50Subtype'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


            label_map = {0: 'class1', 1: 'class2', 2: 'class3'}

            # Loop to process datasets 1 and 2
            for i in range(1, 3):

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer


            # Loop through datasets 1 and 2
            for i in range(1, 3):
                # LabelBinarizer for the one-hot encoding
                lb = LabelBinarizer()
                lb.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lb.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lb.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For 5-label encoding using LabelBinarizer
                lb5 = LabelBinarizer()
                lb5.fit(globals()[f'YTrainCLM_{i}'])

                globals()[f'YTrainCLMHE_{i}'] = lb5.transform(globals()[f'YTrainCLM_{i}'])
                globals()[f'YTestCLMHE_{i}'] = lb5.transform(globals()[f'YTestCLM_{i}'])  # This may change YTest

                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()



            # Dictionary to map numerical labels to string labels
            label_mapping = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 3):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)

            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)

            YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTestCLM_2)
            #one-hot-encoded
            YTestCLMHE_2 = lblNW.transform(YTestCLM_2)

        elif input_type == 'Cathgen':
            scaler = MinMaxScaler()
            X1 = pd.DataFrame(scaler.fit_transform(X1), columns=X1.columns)
            X2 = pd.DataFrame(scaler.fit_transform(X2), columns=X2.columns)
            min_samples = min(X1.shape[0], X2.shape[0])
            X1 = X1.iloc[:min_samples]  # Truncate to the minimum number of samples
            X2 = X2.iloc[:min_samples]  # Truncate to the minimum number of samples
            y1 = y1.iloc[:min_samples]  # Truncate to the minimum number of samples
            y2 = y2.iloc[:min_samples]  # Truncate to the minimum number of samples

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1, X2]
            y_datasets = [y1, y2]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['cadStatus'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary

            label_map = {0: 'class1', 1: 'class2'}
            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Replace the labels using the mapping
                globals()[f'YTest_{i}'] = globals()[f'YTest_{i}'].replace(label_map)
                globals()[f'YTrain_{i}'] = globals()[f'YTrain_{i}'].replace(label_map)

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'class1': 'Class1','class2': 'Class2'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 3):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values
                #LabelToGroup = {str(k): v for k, v in LabelToGroup.items()}

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer



            # Loop through datasets 1 and 2
            for i in range(1, 3):
                # LabelBinarizer for the one-hot encoding
                lbl = LabelBinarizer()
                lbl.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lbl.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lbl.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For label encoding using LabelBinarizer
                lblNW = LabelBinarizer()
                lblNW.fit(YTrainCLM_1)
                #one-hot-encoded
                YTrainCLMHE_1 = lblNW.transform(YTrainCLM_1)

                #################
                # Unique to 2 labels

                # Initialize OneHotEncoder
                ohe = OneHotEncoder(sparse_output=False)
                # Fit and transform the data to one-hot encoding
                YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
                #################
                lb5_2 = LabelBinarizer()
                lb5_2.fit(YTrainCLM_2)

                YTrainCLMHE_2 = lb5_2.transform(YTrainCLM_2)
                YTestCLMHE_2 = lb5_2.transform(YTestCLM_2) # this transformation changes YTest

                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()

          # Dictionary to map numerical labels to string labels
            label_mapping = {'class1': 'Class1', 'class2': 'Class2'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 3):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)

            # Apply the mapping
            string_labels_2 = map_labels(YTestM_series_2)



            for i in range(1, 3):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values


            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################
            lblNW = LabelBinarizer()
            lblNW.fit(YTrainCLM_2)
            #one-hot-encoded
            YTrainCLMHE_2 = lblNW.transform(YTrainCLM_2)
        else:
            raise ValueError("Invalid input type.")

        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def MMVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)
            # Dataset 2
            xe_2 = Dense(num_neurons, activation=f_activation, name='encoder_input_2')(x_ini_2)
            xe_2 = Dropout(k, name='encoder_dropout_2')(xe_2)


            ############################

            ##############################

            CL_1 = Concatenate(axis=-1)([xe_1, xe_2])
            CL_2 = Concatenate(axis=-1)([xe_2, xe_1])


            CL_E_1 = Dense(num_neurons, activation=f_activation)(CL_1)
            CL_E_1 = Dropout(k)(CL_E_1)
            CL_E_2 = Dense(num_neurons, activation=f_activation)(CL_2)
            CL_E_2 = Dropout(k)(CL_E_2)
            ##############################

            # Concatenate Step 2

            CL_F = Concatenate(axis=-1)([CL_E_1, CL_E_2])

            xe = Dense(num_neurons, activation=f_activation)(CL_F)
            xe = Dropout(k)(xe)

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)


            xd3 = Dense(num_neurons, activation=f_activation)(xd1)
            xd4 = Dropout(k)(xd3)

            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)
            xd_ds2 = Dense(num_neurons, activation=f_activation, name='decoder_output_2')(xd4)
            xd_ds2 = Dropout(k, name='decoder_dropout_2')(xd_ds2)




            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)
            xhat_2 = Dense(output_dim_2, activation='sigmoid')(xd_ds2)



            return Model(inputs=[x_ini_1, x_ini_2, labels], outputs=[xhat_1,xhat_2, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, xhat_2, z_mu, z_log_sigma = x_pred
            x_true_1, x_true_2 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)
            x_true_2 = tf.cast(x_true_2, tf.float32)

            #x_true_1, x_true_2 = tf.cast([x_true_1,x_true_2], tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            E_log_px_given_z_DS2 = tf.reduce_sum(tf.square(x_true_2 - xhat_2), axis=1)

            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)
            BCE_2 = tf.reduce_sum(E_log_px_given_z_DS2)

            return KLD + BCE_1 + BCE_2, KLD, BCE_1, BCE_2

        for i in range(1, 3):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons


        # Configure the model
        model = MMVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop
        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}

        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2
        # Custom training loop

        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_xs_2 = xtraining_2[step * batch_size: (step + 1) * batch_size]
                batch_xs1_2 = xtraining_2_2[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, xhat_2, z_mu, z_log_sigma = model([batch_xs, batch_xs_2, batch_labels], training=True)
                    loss_value, KLD, BCE_1, BCE_2 = cvae_loss([batch_xs1, batch_xs1_2], [xhat_1, xhat_2, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))



        xtraining_1 = XTrainM_1
        xtraining_2 = XTrainM_2
        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2

        _,_, z_mu_train, z_log_sigma_train = model.predict([xtraining_1,xtraining_2, YTrainCLMHE_1])
        _,_, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, xtesting_2, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)
           # We save the latent space for the specific method because the same name was used for the other methods
        z_mu_train_df.to_csv('train_latent_space_MMVAE.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space_MMVAE.csv', index=False)


        YTrain = pd.DataFrame(YTrainM_1)
        YTrain.columns = ['class']
        YTrain.to_csv('YTrain_CLS.csv', index=False)

        # Save latent space to CSV files
        YTest = pd.DataFrame(YTestM_1)
        YTest.columns = ['class']
        YTest.to_csv('YTest_CLS.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df


        xtesting_1 = XTestM_1
        xtesting_2 = XTestM_2


        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model

        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train['class']) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test['class'])

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        from sklearn.linear_model import LogisticRegression

        unique_classes = y_train['class'].unique()

        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train['class']) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test['class'], y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test['class'], y_pred_LR)

        accuracy_LR = accuracy_score(y_test['class'], y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test['class'], y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)

    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)


    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)

    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from MM VAE Architecture. "
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results



##########################
##########################
##########################
##########################
#C XVAE for single dataset
##########################
##########################
##########################
##########################

def CXVAESD(dataset, target,n_runs, input_type,labels_dim, num_hidden_vars, num_neurons, batch_size):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset)
        y1 = pd.read_csv(target)

        if input_type == 'PAM50':
            indices_to_remove = y1[y1['Pam50Subtype'] == '?'].index

            # Remove corresponding data from X1, y1, y2, and X2
            X1 = X1.drop(indices_to_remove)
            y1 = y1.drop(indices_to_remove)

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)


            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1]
            y_datasets = [y1]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['Pam50Subtype'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


            label_map = {0: 'class1', 1: 'class2', 2: 'class3'}

            # Loop to process datasets 1 and 2
            for i in range(1, 2):

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 2):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if x == 'LumA' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['Pam50Subtype'].apply(lambda x: 'LumA' if 'LumA' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer


            # Loop through datasets 1 and 2
            for i in range(1, 2):
                # LabelBinarizer for the one-hot encoding
                lb = LabelBinarizer()
                lb.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lb.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lb.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For 5-label encoding using LabelBinarizer
                lb5 = LabelBinarizer()
                lb5.fit(globals()[f'YTrainCLM_{i}'])

                globals()[f'YTrainCLMHE_{i}'] = lb5.transform(globals()[f'YTrainCLM_{i}'])
                globals()[f'YTestCLMHE_{i}'] = lb5.transform(globals()[f'YTestCLM_{i}'])  # This may change YTest

                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()



            # Dictionary to map numerical labels to string labels
            label_mapping = {'LumA': 'LumA','LumB': 'LumB','Basal': 'Basal','Her2': 'Her2','Normal': 'Normal'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 2):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)



            for i in range(1, 2):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
            #################


            YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature

        elif input_type == 'Cathgen':
            scaler = MinMaxScaler()
            X1 = pd.DataFrame(scaler.fit_transform(X1), columns=X1.columns)

            min_samples = min(X1.shape[0], y1.shape[0])
            X1 = X1.iloc[:min_samples]  # Truncate to the minimum number of samples
            y1 = y1.iloc[:min_samples]  # Truncate to the minimum number of samples


            # Reset the index if needed
            y1 = y1.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            # Define your datasets
            X_datasets = [X1]
            y_datasets = [y1]

            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['cadStatus'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary



            label_map = {0: 'class1', 1: 'class2'}

            # Loop to process datasets 1 and 2
            for i in range(1, 2):
                # Replace the labels using the mapping
                globals()[f'YTest_{i}'] = globals()[f'YTest_{i}'].replace(label_map)
                globals()[f'YTrain_{i}'] = globals()[f'YTrain_{i}'].replace(label_map)

                # Convert to matrix
                globals()[f'XTrainM_{i}'] = globals()[f'XTrain_{i}'].values
                globals()[f'XTestM_{i}'] = globals()[f'XTest_{i}'].values

                # One hot encode Y and convert to matrix
                globals()[f'YTrainM_{i}'] = globals()[f'YTrain_{i}'].values
                globals()[f'YTestM_{i}'] = globals()[f'YTest_{i}'].values

                # Get number of features
                globals()[f'num_features_{i}'] = globals()[f'XTrainM_{i}'].shape[1]

            LabelToGroup = {'class1': 'Class1','class2': 'Class2'}  # Ensure 'normal' is included

            # Loop to process datasets 1 and 2
            for i in range(1, 2):
                # Apply the lambda function to categorize the target variable into 'class1' or 'anomaly'
                globals()[f'YTestAN_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestAN_{i}'].values

                globals()[f'YTrainAN_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'class1' if x == 'class1' else 'anomaly')
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainAN_{i}'].values
                #LabelToGroup = {str(k): v for k, v in LabelToGroup.items()}

                # Apply LabelToGroup mapping for more detailed class labeling
                globals()[f'YTestCL_{i}'] = globals()[f'YTest_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = globals()[f'YTrain_{i}']['cadStatus'].apply(lambda x: 'Class1' if 'class1' in x else LabelToGroup[x])
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            from sklearn.preprocessing import LabelBinarizer



            # Loop through datasets 1 and 2
            for i in range(1, 2):
                # LabelBinarizer for the one-hot encoding
                lbl = LabelBinarizer()
                lbl.fit(globals()[f'YTrainM_{i}'])

                # One-hot encoding for training and test labels
                globals()[f'YTrainMHE_{i}'] = lbl.transform(globals()[f'YTrainM_{i}'])
                globals()[f'YTestMHE_{i}'] = lbl.transform(globals()[f'YTestM_{i}'])  # This may change YTest, as it might have labels not present in YTrain

                # For label encoding using LabelBinarizer
                lblNW = LabelBinarizer()
                lblNW.fit(YTrainCLM_1)
                #one-hot-encoded
                YTrainCLMHE_1 = lblNW.transform(YTrainCLM_1)

                #################
                # Unique to 2 labels

                # Initialize OneHotEncoder
                ohe = OneHotEncoder(sparse_output=False)
                # Fit and transform the data to one-hot encoding
                YTestCLMHE_1 = ohe.fit_transform(YTestCLM_1.reshape(-1, 1))  # Reshape needed for a single feature
                #################


                # For 2-label encoding (anomaly detection: class1 vs anomaly)
                globals()[f'YTrainCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTrainCLM_{i}']])
                globals()[f'YTestCLHE_{i}'] = np.array([[1, 0] if x == 'class1' else [0, 1] for x in globals()[f'YTestCLM_{i}']])

                # Convert YTestM to a Series
                globals()[f'YTestM_series_{i}'] = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Get the class counts for training data (if needed)
                globals()[f'class_counts_{i}'] = globals()[f'YTrain_{i}'].value_counts()

          # Dictionary to map numerical labels to string labels
            label_mapping = {'class1': 'Class1', 'class2': 'Class2'}

            # Function to map numerical labels to string labels
            def map_labels(labels):
                return labels.map(label_mapping)


            # Loop through both datasets (1 and 2)
            for i in range(1, 2):
                # Convert YTestM and YTrainM to Pandas Series (flattened)
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())

                # Apply the label mapping
                string_labels_test = map_labels(YTestM_series)
                string_labels_train = map_labels(YTrainM_series)

                # Store the transformed labels for YTest and YTrain
                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values

                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values


            # Apply the mapping
            string_labels_1 = map_labels(YTestM_series_1)


            for i in range(1, 2):
                # Convert YTrainM and YTestM to Pandas Series (flattened)
                YTrainM_series = pd.Series(globals()[f'YTrainM_{i}'].flatten())
                YTestM_series = pd.Series(globals()[f'YTestM_{i}'].flatten())

                # Apply the label mapping
                string_labels_train = map_labels(YTrainM_series)
                string_labels_test = map_labels(YTestM_series)

                # Store the transformed labels for YTrain and YTest
                globals()[f'YTrainCL_{i}'] = string_labels_train
                globals()[f'YTrainCLM_{i}'] = globals()[f'YTrainCL_{i}'].values

                globals()[f'YTestCL_{i}'] = string_labels_test
                globals()[f'YTestCLM_{i}'] = globals()[f'YTestCL_{i}'].values


            ohe = OneHotEncoder(sparse_output=False)

            # Fit and transform the data to one-hot encoding
            YTrainCLMHE_1 = ohe.fit_transform(YTrainCLM_1.reshape(-1, 1))  # Reshape needed for a single feature

        else:
            raise ValueError("Invalid input type.")





        class BatchTraining(object):

            def __init__(self, X, batch_size=100):
                self.X = X
                self.batch_size = batch_size
                self.batch_num = self._calculate_batch_num()
                self.indices = np.arange(self.X.shape[0])

            def _calculate_batch_num(self):
                return int(self.X.shape[0] / self.batch_size)

            def gen_offsets(self, ini=0):
                """Generate batch offsets with a specific starting point"""
                self.indices = np.roll(self.indices, -ini)

            def gen_offsets_random(self, ini=0):
                """Generate random batch offsets with a specific starting point"""
                np.random.shuffle(self.indices)

            def next_batch(self):
                """Return the next batch and update indices"""
                fold = self.indices[:self.batch_size]
                x_batch = self.X[fold, :]
                self.indices = np.roll(self.indices, -self.batch_size)
                return x_batch, fold

            def get_batch_num(self):
                """Return the number of batches"""
                return self.batch_num



        # Define the neural network configuration function
        def CVAE_Archi(num_hidden_vars, f_activation='relu'):
            x_ini_1 = Input(shape=(input_dim_1,))
            #x_ini_2 = Input(shape=(input_dim_2,))
            labels = Input(shape=(labels_dim,))

            k = 0.0001  # dropout probability
            ############################
            ############################
            # Encoder
            ############################
            ############################

            # Dataset 1
            xe_1 = Dense(num_neurons, activation=f_activation, name='encoder_input_1')(x_ini_1)
            xe_1 = Dropout(k, name='encoder_dropout_1')(xe_1)


            ############################

            ##############################


            xe = Concatenate(axis=1)([xe_1, labels])

            xe = Dense(num_neurons, activation=f_activation)(xe)
            xe = Dropout(k)(xe)

            # Posterior distributions
            z_mu = Dense(num_hidden_vars, activation='linear')(xe)
            z_log_sigma = Dense(num_hidden_vars, activation='linear')(xe)

            # Sample epsilon
            epsilon = layers.Lambda(lambda x: tf.random.normal(tf.shape(x)))(z_log_sigma)

            # Sample from posterior
            z = layers.Lambda(lambda args: args[0] + tf.exp(args[1]) * args[2])([z_mu, z_log_sigma, epsilon])

            ############################
            ############################
            # Decoder
            ############################
            ############################
            xd = Dense(num_neurons, activation=f_activation)(z)  # fully-connected layer
            xd1 = Dropout(k)(xd)



            xd2 = Concatenate(axis=1)([xd1, labels])

            xd3 = Dense(num_neurons, activation=f_activation)(xd2)
            xd4 = Dropout(k)(xd3)


            # Braching decoder
            xd_ds1 = Dense(num_neurons, activation=f_activation, name='decoder_output_1')(xd4)
            xd_ds1 = Dropout(k, name='decoder_dropout_1')(xd_ds1)

            # Posterior distributions
            xhat_1 = Dense(output_dim_1, activation='sigmoid')(xd_ds1)




            return Model(inputs=[x_ini_1, labels], outputs=[xhat_1, z_mu, z_log_sigma])



        # Define the custom loss function
        def cvae_loss(x_true, x_pred):
            xhat_1, z_mu, z_log_sigma = x_pred
            x_true_1 = x_true
            x_true_1 = tf.cast(x_true_1, tf.float32)

            E_log_px_given_z_DS1 = tf.reduce_sum(tf.square(x_true_1 - xhat_1), axis=1)


            kl_div = -0.5 * tf.reduce_sum(
                1.0 + 2.0 * z_log_sigma - tf.square(z_mu) - tf.exp(2.0 * z_log_sigma),
                axis=1)

            KLD = tf.reduce_mean(kl_div)
            BCE_1 = tf.reduce_sum(E_log_px_given_z_DS1)

            return KLD + BCE_1, KLD, BCE_1

        for i in range(1, 2):
            # Dynamically assign variables for training and testing datasets
            globals()[f'xtraining_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}'] = globals()[f'XTestM_{i}']
            globals()[f'xtraining_{i}_{i}'] = globals()[f'XTrainM_{i}']
            globals()[f'xtesting_{i}_{i}'] = globals()[f'XTestM_{i}']

            # Assign input and output dimensions
            globals()[f'input_dim_{i}'] = globals()[f'xtraining_{i}'].shape[1]
            globals()[f'output_dim_{i}'] = globals()[f'xtraining_{i}_{i}'].shape[1]



        labels_dim = labels_dim
        label_repeat = 1
        num_hidden_vars = num_hidden_vars
        f_activation = 'relu'
        num_neurons = num_neurons


        # Configure the model
        model = CVAE_Archi(num_hidden_vars, f_activation)
        optimizer = Adam(learning_rate=0.0002)

        # Custom training loop
        batch_size = batch_size
        n_epochs = 20
        print_step = 50

        history = {'cost_h': [], 'train_rmse': [], 'test_rmse': [], 'kld': [], 'bce_1': [], 'bce_2': []}
        xtraining_1 = XTrainM_1
        xtesting_1 = XTestM_1


        # Custom training loop

        for epoch in range(n_epochs):
            bt = BatchTraining(xtraining_1, batch_size)

            # Generate random offset for shuffling
            ini_offset = np.random.randint(0, batch_size, 1)[0]
            bt.gen_offsets(ini_offset)

            num_batches = bt.get_batch_num()

            for step in range(num_batches):
                batch_xs, _ = bt.next_batch()
                batch_xs1 = xtraining_1_1[step * batch_size: (step + 1) * batch_size]
                batch_labels = YTrainCLMHE_1[step * batch_size: (step + 1) * batch_size]

                with tf.GradientTape() as tape:
                    xhat_1, z_mu, z_log_sigma = model([batch_xs, batch_labels], training=True)
                    loss_value, KLD, BCE_1= cvae_loss([batch_xs1], [xhat_1, z_mu, z_log_sigma])

                grads = tape.gradient(loss_value, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))



        xtraining_1 = XTrainM_1
        xtesting_1 = XTestM_1


        _, z_mu_train, z_log_sigma_train = model.predict([xtraining_1, YTrainCLMHE_1])
        _, z_mu_test, z_log_sigma_test = model.predict([xtesting_1, YTestCLMHE_1])

        # Save the latent variables to CSV files
        z_mu_train_df = pd.DataFrame(z_mu_train)
        z_log_sigma_train_df = pd.DataFrame(z_log_sigma_train)
        z_mu_test_df = pd.DataFrame(z_mu_test)
        z_log_sigma_test_df = pd.DataFrame(z_log_sigma_test)


        print("Latent space saved to CSV files.")

        # Save latent space to CSV files
        z_mu_train_df.to_csv('train_latent_space.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space.csv', index=False)
           # We save the latent space for the specific method because the same name was used for the other methods
        z_mu_train_df.to_csv('train_latent_space_CXVAESD.csv', index=False)
        z_mu_test_df.to_csv('test_latent_space_CXVAESD.csv', index=False)


        YTrain = pd.DataFrame(YTrainM_1)
        YTrain.columns = ['class']
        YTrain.to_csv('YTrain_CLS.csv', index=False)

        # Save latent space to CSV files
        YTest = pd.DataFrame(YTestM_1)
        YTest.columns = ['class']
        YTest.to_csv('YTest_CLS.csv', index=False)


        YTest_CLS = YTest
        YTrain_CLS = YTrain
        test_latent_space = z_mu_test_df
        train_latent_space = z_mu_train_df




        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model

        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train['class']) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test['class'])

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS
        from sklearn.linear_model import LogisticRegression

        unique_classes = y_train['class'].unique()

        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train['class']) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test['class'], y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test['class'], y_pred_LR)

        accuracy_LR = accuracy_score(y_test['class'], y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test['class'], y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = test_latent_space
        X_train = train_latent_space
        y_test= YTest_CLS
        y_train = YTrain_CLS

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)




    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)


    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from CXVAE Architecture for single dataset. "
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results


##########################
##########################
##########################
##########################
#Concatenation
##########################
##########################
##########################
##########################


def CONCAT(dataset1, dataset2, target,n_runs, input_type,labels_dim,YColName):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)

        if input_type == 'PAM50':
          # Scale features
          indices_to_remove = y1[y1['Pam50Subtype'] == '?'].index

          # Remove corresponding data from X1, y1, y2, and X2
          X1 = X1.drop(indices_to_remove)
          y1 = y1.drop(indices_to_remove)
          X2 = X2.drop(indices_to_remove)
          y2 = y2.drop(indices_to_remove)

          # Reset the index if needed
          y1 = y1.reset_index(drop=True)
          y2 = y2.reset_index(drop=True)

          from sklearn.model_selection import train_test_split


          # Define your datasets
          X_Concat = pd.concat([X1, X2], axis=1)
          X_datasets = [X_Concat]
          y_datasets = [y1]

          # Dictionaries to store the train and test sets with indices starting from 1
          XTrain = {}
          XTest = {}
          YTrain = {}
          YTest = {}

          # Loop through each dataset and split into train and test sets
          for i in range(len(X_datasets)):
              X_train, X_test, y_train, y_test = train_test_split(
                  X_datasets[i],
                  y_datasets[i],
                  test_size=0.3,
                  stratify=y_datasets[i]['Pam50Subtype'],
                  random_state=seed
              )

              # Store the results in dictionaries, using i+1 as the key
              # Store the results in dictionaries using i+1 as the key to match numbering
              XTrain[f'XTrain_{i+1}'] = X_train
              XTest[f'XTest_{i+1}'] = X_test
              YTrain[f'YTrain_{i+1}'] = y_train
              YTest[f'YTest_{i+1}'] = y_test

          for i in range(1, len(XTrain) + 1):
              globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
              globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
              globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
              globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary

          YTest_1.columns = ['class']
          YTrain_1.columns = ['class']
          YTest_CLS = YTest_1
          YTrain_CLS = YTrain_1


        elif input_type == 'Cathgen':
            scaler = MinMaxScaler()
            X1 = pd.DataFrame(scaler.fit_transform(X1), columns=X1.columns)
            X2 = pd.DataFrame(scaler.fit_transform(X2), columns=X2.columns)
            min_samples = min(X1.shape[0], X2.shape[0])
            X1 = X1.iloc[:min_samples]  # Truncate to the minimum number of samples
            X2 = X2.iloc[:min_samples]  # Truncate to the minimum number of samples
            y1 = y1.iloc[:min_samples]  # Truncate to the minimum number of samples
            y2 = y2.iloc[:min_samples]  # Truncate to the minimum number of samples

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            X_Concat = pd.concat([X1, X2], axis=1)
            X_datasets = [X_Concat]
            y_datasets = [y1]
            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['cadStatus'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


            YTest_CLS = YTest_1
            YTrain_CLS = YTrain_1


        else:
            raise ValueError("Invalid input type.")




        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model

        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train[YColName]) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test[YColName])

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS
        from sklearn.linear_model import LogisticRegression

        unique_classes = y_train[YColName].unique()

        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train[YColName]) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test[YColName], y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test[YColName], y_pred_LR)

        accuracy_LR = accuracy_score(y_test[YColName], y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test[YColName], y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)


    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)



    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)


    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)


    results = {
        "Results from Concatenation. "
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results




##########################
##########################
##########################
##########################
#PCA Both
##########################
##########################
##########################
##########################


def PCA(dataset1, dataset2, target,n_runs, input_type,labels_dim,YColName):
    import pandas as pd

    import random

    random.seed(42)

    # Import libraries
    import numpy as np

    import matplotlib.pyplot as plt
    import tensorflow as tf
    import keras
    from keras import backend as K
    from keras.layers import Dense, Dropout
    from math import sqrt
    from six.moves import cPickle as pickle

    # Scikit-learn imports
    from sklearn import svm
    from sklearn.preprocessing import (
        MultiLabelBinarizer, LabelBinarizer, LabelEncoder, OneHotEncoder,
        StandardScaler, MinMaxScaler
    )
    from sklearn.kernel_approximation import RBFSampler
    from sklearn.linear_model import LogisticRegression, SGDClassifier
    from sklearn.multiclass import OneVsRestClassifier
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
        roc_curve, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error,
        precision_recall_fscore_support
    )
    from sklearn.model_selection import train_test_split, GridSearchCV

    # TensorFlow/Keras imports
    from tensorflow.keras.layers import (
        BatchNormalization as BN, Concatenate, Dense, Dropout, Input, Lambda
    )
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Adam

    # Pandas-specific import
    import pandas.core.algorithms as algos
    from pandas import Series
    from tensorflow.keras.layers import Layer
    from tensorflow.keras import layers

    accuracy_scores_RF = []
    F1_scores_RF = []

    accuracy_scores_NN = []
    F1_scores_NN = []

    accuracy_scores_LR = []
    F1_scores_LR = []


    accuracy_scores_SVM = []
    F1_scores_SVM = []


    accuracy_scores_SVM_NL = []
    F1_scores_SVM_NL = []

    accuracy_scores_NB = []
    F1_scores_NB = []



    for i in range(n_runs):
        # Set a new random seed for each iteration
        seed = 42 + i
        np.random.seed(seed)
        random.seed(seed)
        tf.random.set_seed(seed)

        # Reload the datasets
        X1 = pd.read_csv(dataset1)
        X2 = pd.read_csv(dataset2)
        y1 = pd.read_csv(target)
        y2 = pd.read_csv(target)

        if input_type == 'PAM50':
          # Scale features
          indices_to_remove = y1[y1['Pam50Subtype'] == '?'].index

          # Remove corresponding data from X1, y1, y2, and X2
          X1 = X1.drop(indices_to_remove)
          y1 = y1.drop(indices_to_remove)
          X2 = X2.drop(indices_to_remove)
          y2 = y2.drop(indices_to_remove)

          # Reset the index if needed
          y1 = y1.reset_index(drop=True)
          y2 = y2.reset_index(drop=True)

          from sklearn.model_selection import train_test_split

          # Define your datasets
          X_Concat = pd.concat([X1, X2], axis=1)
          X_datasets = [X_Concat]
          y_datasets = [y1]
          XTrain = {}
          XTest = {}
          YTrain = {}
          YTest = {}

          # Loop through each dataset and split into train and test sets
          for i in range(len(X_datasets)):
              X_train, X_test, y_train, y_test = train_test_split(
                  X_datasets[i],
                  y_datasets[i],
                  test_size=0.3,
                  stratify=y_datasets[i]['Pam50Subtype'],
                  random_state=seed
              )

          # Store the results in dictionaries, using i+1 as the key
          # Store the results in dictionaries using i+1 as the key to match numbering
          XTrain[f'XTrain_{i+1}'] = X_train
          XTest[f'XTest_{i+1}'] = X_test
          YTrain[f'YTrain_{i+1}'] = y_train
          YTest[f'YTest_{i+1}'] = y_test

          for i in range(1, len(XTrain) + 1):
              globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
              globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
              globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
              globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary


          from sklearn.decomposition import PCA


          # Apply PCA on the training set
          pca = PCA(n_components=0.95)  # Retain 95% of variance
          X_train_pca = pca.fit_transform(XTrain_1)
          X_test_pca = pca.transform(XTest_1)

          train_latent_space = X_train_pca
          test_latent_space = X_test_pca
          YTest_1.columns = ['class']
          YTrain_1.columns = ['class']
          YTest_CLS = YTest_1
          YTrain_CLS = YTrain_1

        elif input_type == 'Cathgen':
            scaler = MinMaxScaler()
            X1 = pd.DataFrame(scaler.fit_transform(X1), columns=X1.columns)
            X2 = pd.DataFrame(scaler.fit_transform(X2), columns=X2.columns)
            min_samples = min(X1.shape[0], X2.shape[0])
            X1 = X1.iloc[:min_samples]  # Truncate to the minimum number of samples
            X2 = X2.iloc[:min_samples]  # Truncate to the minimum number of samples
            y1 = y1.iloc[:min_samples]  # Truncate to the minimum number of samples
            y2 = y2.iloc[:min_samples]  # Truncate to the minimum number of samples

            # Reset the index if needed
            y1 = y1.reset_index(drop=True)
            y2 = y2.reset_index(drop=True)

            from sklearn.model_selection import train_test_split

            X_Concat = pd.concat([X1, X2], axis=1)
            X_datasets = [X_Concat]
            y_datasets = [y1]
            # Dictionaries to store the train and test sets with indices starting from 1
            XTrain = {}
            XTest = {}
            YTrain = {}
            YTest = {}

            # Loop through each dataset and split into train and test sets
            for i in range(len(X_datasets)):
                X_train, X_test, y_train, y_test = train_test_split(
                    X_datasets[i],
                    y_datasets[i],
                    test_size=0.3,
                    stratify=y_datasets[i]['cadStatus'],
                    random_state=seed
                )

                # Store the results in dictionaries, using i+1 as the key
                # Store the results in dictionaries using i+1 as the key to match numbering
                XTrain[f'XTrain_{i+1}'] = X_train
                XTest[f'XTest_{i+1}'] = X_test
                YTrain[f'YTrain_{i+1}'] = y_train
                YTest[f'YTest_{i+1}'] = y_test

            for i in range(1, len(XTrain) + 1):
                globals()[f'XTrain_{i}'] = XTrain[f'XTrain_{i}']
                globals()[f'XTest_{i}'] = XTest[f'XTest_{i}'] # Accessing the correct dictionary
                globals()[f'YTrain_{i}'] = YTrain[f'YTrain_{i}'] # Accessing the correct dictionary
                globals()[f'YTest_{i}'] = YTest[f'YTest_{i}'] # Accessing the correct dictionary
            from sklearn.decomposition import PCA
            pca = PCA(n_components=0.95)  # Retain 95% of variance
            X_train_pca = pca.fit_transform(XTrain_1)
            X_test_pca = pca.transform(XTest_1)

            train_latent_space = X_train_pca
            test_latent_space = X_test_pca

            YTest_CLS = YTest_1
            YTrain_CLS = YTrain_1



        else:
            raise ValueError("Invalid input type.")




        ############################
        ###########################
        #Two Step Prediction
        ############################
        ############################



        ############################
        # Randon Forest Decision tree
        ############################

        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
        from sklearn.tree import plot_tree
        import matplotlib.pyplot as plt
        from sklearn.impute import SimpleImputer
        from sklearn.ensemble import RandomForestClassifier

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS
        classifier = RandomForestClassifier(
            n_estimators=100,  # Number of trees in the forest
            max_depth=None,    # Maximum depth of the tree
            min_samples_split=2,  # Minimum number of samples required to split an internal node
            random_state=42
        )
        classifier.fit(X_train, y_train)

        # Make predictions and evaluate the model
        y_pred_RF = classifier.predict(X_test)

        # Evaluate the model

        conf_matrix = confusion_matrix(y_test, y_pred_RF)

        accuracy_RF = accuracy_score(y_test, y_pred_RF)
        accuracy_scores_RF.append(accuracy_RF)

        F1_score_RF = f1_score(y_test, y_pred_RF, average='weighted')
        F1_scores_RF.append(F1_score_RF)



        ############################
        # Neural Network
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS
        np.random.seed(1)            # NumPy random seed
        tf.random.set_seed(1)         # TensorFlow random seed
        random.seed(1)

        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()

        # Standardize data

        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.fit_transform(X_test)

        # Convert string labels to numerical representations
        label_encoder = LabelEncoder()
        y_train_encoded = label_encoder.fit_transform(y_train[YColName]) # Encode labels in y_train

        # Define the neural network model
        def create_model(input_shape):
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(1024, activation='relu', input_shape=input_shape),
                tf.keras.layers.Dense(512, activation='relu'),
                tf.keras.layers.Dense(labels_dim, activation='softmax')
            ])
            model.compile(optimizer='adam',
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])
            return model

        # Train and evaluate the model for X1
        model_X1 = create_model((X_train_scaled.shape[1],))
        history_X1 = model_X1.fit(X_train_scaled, y_train_encoded, epochs=30, batch_size=128, # Use encoded labels for training
                              validation_split=0.2, verbose=0)


        # Evaluate the models
        Y1_pred = model_X1.predict(X_test_scaled)

        Y1_pred_classes_NN = np.argmax(Y1_pred, axis=1)

        # Encode labels in y_test
        y_test_encoded = label_encoder.transform(y_test[YColName])

        # Print confusion matrices
        conf_matrix_X1 = confusion_matrix(y_test_encoded, Y1_pred_classes_NN) # Use encoded labels for evaluation

        accuracy_NN = accuracy_score(y_test_encoded, Y1_pred_classes_NN)
        accuracy_scores_NN.append(accuracy_NN)

        F1_NN = f1_score(y_test_encoded, Y1_pred_classes_NN, average='weighted')
        F1_scores_NN.append(F1_NN)


        ############################
        # Logistic Regaression
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS
        from sklearn.linear_model import LogisticRegression

        unique_classes = y_train[YColName].unique()

        # Create a logistic regression model
        model = LogisticRegression(max_iter=1000,multi_class='multinomial', solver='lbfgs')

        # Train the model
        model.fit(X_train, y_train[YColName]) # Fit on the 'class' column

        # Make predictions on the test set
        y_pred_LR = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test[YColName], y_pred_LR) # Evaluate on the 'class' column
        conf_matrix_LR = confusion_matrix(y_test[YColName], y_pred_LR)

        accuracy_LR = accuracy_score(y_test[YColName], y_pred_LR)
        accuracy_scores_LR.append(accuracy_LR)

        F1_LR = f1_score(y_test[YColName], y_pred_LR, average='weighted')
        F1_scores_LR.append(F1_LR)

        ############################
        # SVM
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier = SVC(kernel='linear', random_state=42)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM = svm_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)

        accuracy_SVM = accuracy_score(y_test, y_pred_SVM)
        accuracy_scores_SVM.append(accuracy_SVM)

        F1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')
        F1_scores_SVM.append(F1_SVM)

        ############################
        # SVM Non Linear
        ############################

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS

        from sklearn.svm import SVC


        # Initialize the SVM classifier
        svm_classifier_NL = SVC(kernel='rbf', random_state=42,gamma= "auto", C=1.5)  # You can try different kernels (linear, rbf, poly)

        # Train the model
        svm_classifier_NL.fit(X_train, y_train)

        # Make predictions
        y_pred_SVM_NL = svm_classifier_NL.predict(X_test)

        # Evaluate the model

        conf_matrix_SVM_NL = confusion_matrix(y_test, y_pred_SVM_NL)

        accuracy_SVM_NL = accuracy_score(y_test, y_pred_SVM_NL)
        accuracy_scores_SVM_NL.append(accuracy_SVM_NL)

        F1_SVM_NL = f1_score(y_test, y_pred_SVM_NL, average='weighted')
        F1_scores_SVM_NL.append(F1_SVM_NL)

        ############################
        # Naive Bayes
        ############################
        from sklearn.naive_bayes import GaussianNB

        X_test = XTest_1
        X_train = XTrain_1
        y_test= YTest_CLS
        y_train = YTrain_CLS

        nb_classifier = GaussianNB()

        # Train the model
        nb_classifier.fit(X_train, y_train)

        # Make predictions
        y_pred_NB = nb_classifier.predict(X_test)

        # Evaluate the model

        conf_matrix_NB = confusion_matrix(y_test, y_pred_NB)

        accuracy_NB = accuracy_score(y_test, y_pred_NB)
        accuracy_scores_NB.append(accuracy_NB)

        F1_NB = f1_score(y_test, y_pred_NB, average='weighted')
        F1_scores_NB.append(F1_NB)




    ####################
    ####################
    # Ave and std dev of accuracy - Two step prediction
    ####################
    ####################

    ####################
    # Random forest
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_RF = np.mean(accuracy_scores_RF)
    std_accuracy_RF = np.std(accuracy_scores_RF)/ np.sqrt(n_runs)


    average_F1_RF = np.mean(F1_scores_RF)
    std_F1_RF = np.std(F1_scores_RF)/ np.sqrt(n_runs)


    ####################
    # Neural Network
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NN = np.mean(accuracy_scores_NN )
    std_accuracy_NN  = np.std(accuracy_scores_NN )/ np.sqrt(n_runs)


    average_F1_NN  = np.mean(F1_scores_NN )
    std_F1_NN  = np.std(F1_scores_NN )/ np.sqrt(n_runs)


    ####################
    # Logistic Regression
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_LR = np.mean(accuracy_scores_LR )
    std_accuracy_LR  = np.std(accuracy_scores_LR )/ np.sqrt(n_runs)


    average_F1_LR  = np.mean(F1_scores_LR )
    std_F1_LR  = np.std(F1_scores_LR )/ np.sqrt(n_runs)


    ####################
    # Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM = np.mean(accuracy_scores_SVM )
    std_accuracy_SVM  = np.std(accuracy_scores_SVM )/ np.sqrt(n_runs)


    average_F1_SVM  = np.mean(F1_scores_SVM )
    std_F1_SVM  = np.std(F1_scores_SVM )/ np.sqrt(n_runs)

    ####################
    # Non Linear SVM
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_SVM_NL = np.mean(accuracy_scores_SVM_NL )
    std_accuracy_SVM_NL  = np.std(accuracy_scores_SVM_NL )/ np.sqrt(n_runs)


    average_F1_SVM_NL  = np.mean(F1_scores_SVM_NL )
    std_F1_SVM_NL  = np.std(F1_scores_SVM_NL )/ np.sqrt(n_runs)



    ####################
    # Naive Bayes
    ####################

    # Calculate and print average accuracy and standard deviation
    average_accuracy_NB = np.mean(accuracy_scores_NB )
    std_accuracy_NB  = np.std(accuracy_scores_NB )/ np.sqrt(n_runs)


    average_F1_NB  = np.mean(F1_scores_NB )
    std_F1_NB  = np.std(F1_scores_NB)/ np.sqrt(n_runs)

    results = {
        "Results from PCA. "
        "Average Accuracy- Random Forest": average_accuracy_RF,
        "Standard Error of Accuracy- Random Forest": std_accuracy_RF,
        "Average F1 Score- Random Forest": average_F1_RF,
        "Standard Error of F1 Score- Random Forest": std_F1_RF,
        "Average Accuracy- Neural Network": average_accuracy_NN,
        "Standard Error of Accuracy- Neural Network": std_accuracy_NN,
        "Average F1 Score- Neural Network": average_F1_NN,
        "Standard Error of F1 Score- Neural Network": std_F1_NN,
        "Average Accuracy- Logistic Regression": average_accuracy_LR,
        "Standard Error of Accuracy- Logistic Regression": std_accuracy_LR,
        "Average F1 Score- Logistic Regression": average_F1_LR,
        "Standard Error of F1 Score- Logistic Regression": std_F1_LR,
        "Average Accuracy- Linear SVM": average_accuracy_SVM,
        "Standard Error of Accuracy- Linear SVM": std_accuracy_SVM,
        "Average F1 Score- Linear SVM": average_F1_SVM,
        "Standard Error of F1 Score- Linear SVM": std_F1_SVM,
        "Average Accuracy- Non Linear SVM": average_accuracy_SVM_NL,
        "Standard Error of Accuracy- Non Linear SVM": std_accuracy_SVM_NL,
        "Average F1 Score- Non Linear SVM": average_F1_SVM_NL,
        "Standard Error of F1 Score- Non Linear SVM": std_F1_SVM_NL,
        "Average Accuracy- Naive Bayes": average_accuracy_NB,
        "Standard Error of Accuracy- Naive Bayes": std_accuracy_NB,
        "Average F1 Score- Naive Bayes": average_F1_NB,
        "Standard Error of F1 Score- Naive Bayes": std_F1_NB,
    }

    return results